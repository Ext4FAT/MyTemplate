
                     摘　要 

   近年来，深度摄像头因其功能强大、成本相对较低、部署方便，在室内场 

景建模、机器人自动定位导航等领域得到广泛使用。但是，在利用深度摄像机 

的深度视觉数据来驱动机械臂进行物体抓取方面，由于对精度和实时性有较高 

要求，现有方法较难满足要求。本文对这一问题进行了系统研究，论文选题具 

有较强的理论意义和实用价值。 

   本文提出了一种基于3D点云配准、深度摄像头驱动的机械臂自动物体抓取 

方法。具体来讲，本文结合RGB-D数据的相关特点，对基于视觉驱动的机械臂 

定位与抓取问题进行了综述，梳理出当前机械臂领域的研究与应用现况。之后， 

本文给出了一种新的深度数据驱动的机械臂抓取算法框架，并将其分为区域分 

割、物体分类、点云配准以及机械臂驱动四个算法模块，具体工作包括： 

  1.  预处理与3D抓取区域的标注：使用3D扫描仪对常见桌面级物体进行建模， 

   并在3D点云模型上标注出物体的可抓取部位；之后采集物体的二维图片， 

   以训练分类器； 

  2. 结合深度与颜色信息的物体筛选：利用深度摄像头采集的深度信息对二维 

   图片进行区域分割，再利用预处理时生成的物体分类器进行分割后的区域 

   进行分类，得到物体的二维候选区域； 

  3. 点云配准：利用深度信息和摄像机的内参生成点云数据，进而利用生成的 

   点云数据和扫描物体的点云数据进行点云配准，获取空间的转移矩阵，最 

   终通过转移矩阵将物体的可抓取部分映射回深度相机的三维空间； 

  4. 视觉驱动的机械臂抓取：利用棋盘标定来估算外参后，将相机空间预测的 

   区域转换到机械臂空间，从而驱动机械臂实施抓取任务。 

   本实验采集了五类常见的桌面级物体（瓶子、茶杯、易拉罐、茶壶和盒 

子），分别对其建模并利用提出的算法进行了测试，并利用杰卡德相似度作为 

评判标准。实验结果表明，本文所提出的方法取得了良好的效果。 

关键词：RGB-D; 深度摄像机; 点云配准; 物体抓取 

                       i 

----------------------- Page 5-----------------------

        南南南京京京大大大学学学研研研究究究生生生毕毕毕业业业论论论文文文英英英文文文摘摘摘要要要首首首页页页用用用纸纸纸 

THESIS:  Research on Robotic Grasping Region Localization through RGB- 

             D camera 

SPECIALIZATION:  Computer Science 

POSTGRADUATE:  Yiyang Zhou 

MENTOR:  Professor Tong Lu 

                                         Abstract 

      With the widespread use of depth cameras, RGB-D data based applications have 

grown rapidly in each passing day such as in indoor scene modeling and automatic 

navigation. Meanwhile, price of depth camera is also becoming more and more cheap. 

However,  RGB-D data driven robot grasping still cannot well meet human require- 

ments due to accuracy and efﬁciency problems in vision systems. 

      In this paper, we present a novel framework to drive automatic robotic grasp by 

matching camera captured RGB-D data with 3D meshes, on which prior knowledge 

for grasp is pre-deﬁned for each object type.       And then propose a novel grasping lo- 

calization algorithm based on RGB-D data to drive robot. This proposed framework is 

composed of four parts: region segmentation, object classiﬁcation, points cloud regis- 

tration and robotic driver. 

      This algorithm framework can be summarized in the following sections: 

    1.  Build 3D model and mark grasping region ：scan 3D meshes for typical object 

       shapes and pre-deﬁne grasping regions for each 3D shape surface, which will be 

       considered as the prior knowledge for guiding automatic robotic grasp. 

   2.  Filter object regions based on RGB-D: segment RGB-D image captured by a 

       depth  camera  without  semantics  from  background  using  depth  data,  and  then 

       recognize 2D shape of the object by a SVM classiﬁer. 

   3.  Points  cloud  registration:  propose  a  new  algorithm  to  register  the  segmented 

       points cloud data generated by depth camera inherent parameter with predeﬁned 

       3D meshes to gain transformation matrix, and ﬁnally convert points cloud data 

       of grasping region to camera coordinate. 

                                               ii 

----------------------- Page 6-----------------------

    4.  Robotic drive grasping: estimate the external matrix with chessboard calibration 

        and reﬂect camera coordinate to robotic coordinate. 

      Our experimental results show that the proposed framework is particularly use- 

ful to guide camera based robotic grasp.               At the end of the paper,  we also give the 

demonstration of our automatic grasping region localization system. 

Keywords:       RGB-D, Depth camera, Registration, Robotic grasping 

                                                     iii 

----------------------- Page 7-----------------------

                                      目目目录录录 

目录 ····································································· iv 

第一章 绪论 ···························································· 1 

    1.1  研究背景························································ 1 

    1.2  本文工作························································ 2 

    1.3  论文结构························································ 3 

第二章 研究进展综述 ··················································· 4 

    2.1  机器视觉的发展 ················································· 4 

    2.2  视觉伺服控制的现状············································· 4 

         2.2.1   典型的相机模型·········································· 5 

         2.2.2   视觉信息处理 ············································ 6 

    2.3  机器人研究现状 ················································· 7 

    2.4  物体抓取区域定位研究现状······································ 8 

    2.5  本章小结························································ 9 

第三章 结合深度和颜色信息的场景物体检测 ····························· 10 

    3.1  基于深度图像的区域分割 ········································ 10 

         3.1.1   区域增长算法 ············································ 12 

         3.1.2   区域合并规则 ············································ 12 

    3.2  基于颜色数据的物体分类 ········································ 13 

         3.2.1   方向梯度直方图 ·········································· 13 

         3.2.2   支持向量机 ·············································· 17 

    3.3  实验结果························································ 18 

    3.4  本章小结························································ 19 

第四章 结合RANSAC与ICP的场景物体点云配准························· 20 

    4.1  点云数据生成 ··················································· 20 

                                        iv 

----------------------- Page 8-----------------------

   4.2   快速点特征直方图··············································· 23 

   4.3   结合RANSAC与ICP 的点云配准 ·································· 26 

        4.3.1   随机一致性算法·········································· 27 

        4.3.2   迭代近邻点算法 ·········································· 30 

   4.4   实验结果························································ 31 

   4.5   本章小结························································ 34 

第五章 基于深度相机的机械臂实物抓取系统实现 ························ 35 

   5.1   算法框架························································ 35 

        5.1.1   预处理阶段 ·············································· 35 

        5.1.2   抓取定位阶段 ············································ 37 

   5.2   软硬件实施······················································ 39 

   5.3   坐标变换························································ 40 

        5.3.1   棋盘坐标················································· 41 

        5.3.2   机械臂坐标 ·············································· 41 

        5.3.3   相机坐标················································· 42 

   5.4   抓取实践测试 ··················································· 43 

   5.5   本章小结························································ 44 

第六章 总结与展望······················································ 45 

   6.1   本文成果························································ 45 

   6.2   不足与展望······················································ 45 

参考文献 ································································ 47 

简历与科研成果·························································· 55 

致谢 ····································································· 56 

                                      v 

----------------------- Page 9-----------------------

                                    表表表格格格 

2.1   典型相机模型 ··················································· 6 

2.2   图像特征法分类 ················································· 7 

2.3   多视图几何法分类··············································· 7 

4.1   提出方法与其他基于ICP方法的比较······························ 33 

5.1   抓取系统所使用的器材 ·········································· 39 

5.2   抓取系统所需依赖库············································· 40 

                                      vi 

----------------------- Page 10-----------------------

                                 插插插图图图 

1.1  工业型机器人与服务型机器人···································· 1 

2.1  视觉伺服系统架构 ··············································· 5 

3.1  基于深度图像的区域分割 ········································ 10 

3.2  基于颜色信息的物体分类 ········································ 13 

3.3  HOG特征描述子················································· 14 

3.4  收集物体二维图片样本，抽取物体的HOG特征···················· 16 

3.5  支持向量机(Support Vector Machine)  ······························ 18 

3.6  不同尺度提取HOG 的分类准确率································· 19 

4.1  单目视觉标定 ··················································· 21 

4.2  点云数据的三维特征表示 ········································ 24 

4.3  FPFH 中的d维分离直方图 ········································ 26 

4.4  配准各阶段示意图··············································· 28 

4.5  RANSAC算法可适应含噪音的数据集····························· 28 

4.6  地面实况（Groundtruth ）与配准结果····························· 32 

4.7  评价标准-Jaccard相似度·········································· 33 

5.1  物体抓取区域定位算法整体框架 ································· 36 

5.2  采集到的数据集 ················································· 37 

5.3  在3D 点云模型上标记抓取区域位置······························ 38 

5.4  深度相机与机械臂··············································· 40 

5.5  实际边长为71mm 的5 ×5棋盘 ···································· 41 

5.6  确定机械臂原点坐标············································· 42 

5.7  通过OpenCV找到棋盘上的交接点 ································ 43 

5.8  实体抓取演示效果··············································· 44 

                                   vii 

----------------------- Page 11-----------------------

                       第一章 绪论 

                     第第第一一一章章章 绪绪绪论论论 

1.1 研研研究究究背背背景景景 

   机器人在实际生活中的应用正悄然经历着革命性的变化。由于现阶段机器 

人应用需求的日益复杂，机器人的种类也呈多样化的趋势。粗糙划分的话，我 

们可以将机器人分为工业型机器人和服务型机器人。工业型机器人在日常的生 

产制造中已逐步取代人工重复操作，比如焊接、搬运、装配、喷漆、切割等任 

务，大大节约了人力成本，提高了生产效率。工业型机器人通常和我们印象中 

的机器人不太一样，这类机器人往往是没有类人形的外观，看上去就是一台机 

器。工业型机器人的本质就是为完成特定任务而专门定制的器械。图1.1(a)展示 

的就是德国KUKA公司量产的一款灵敏装配作业的轻型机器人。服务型机器人 

可分为专业服务型和家庭服务型。专业服务型和工业机器人某些特性相似，但 

属于民用化，没有过细的精度要求，例如扫地机器人、医疗服务机器人以及住 

宅安全防御机器人；家庭型主要以娱乐、交互式机器人为主并能完成一些日常 

任务。图1.1(b)是Intel公司开发的Bunnyman ，这是一种交互式问答机器人，主要 

用于家庭服务娱乐。 

       (a)  德国KUKA公司的LBR iiwa       (b)  Intel公司的Bunnyman 

               图1.1: 工业型机器人与服务型机器人 

   现今，工业型机器人的技术已经相当成熟，讲究的是特定场合完成特定任 

务，所需环境是固定的，但随着工业4.0 的到来，工业型机器人也面临着智能化 

                           1 

----------------------- Page 12-----------------------

                   第一章 绪论 

的改革；服务型机器人的定位则是智能化，这就需要在其上开发更多的自主功 

能以适应各种不同的复杂环境。近年来，人工智能的方法在机器人领域的各类 

应用已如火如荼地展开，机器人的发展迎来了大浪潮时代。 

   与此同时，新型的智能感知硬件越来越廉价且性能越来越高，尤其是RGB- 

D深度摄像机的应用与普及。举例来说，利用RGB-D相机（如微软的Kinect [1]和 

英特尔的Realsense[2] ）可以在机器人场景探索物体时，不仅获取颜色信息，还 

能获取物体到相机的距离。RGB-D相机的这种特性在场景感知系统可以使机器 

人更加智能与自主地探索周围环境。因此，大量研究人员都聚焦在基于RGB- 

D相机的机器人自主勘探任务，例如室内场景3D 建模[3]、自我定位[4]、导航 

[5]和路径规划[6]等。 

   但是，和机器人自动勘探任务任务相反的是，少有利用RGB-D数据驱动机 

器人自动抓取定位的相关研究，这可能由于深度相机本身获取的距离的限制， 

大多数深度相机的可测深度范围在1米以外，而自动抓取定位需要较近距离来确 

定物体的位置。另外纯二维视觉来做抓取区域判断有诸多限制，如视角、相机 

与物体距离、光照条件等问题，因而检测真实场景下物体的可抓区域存在许多 

困难。相对而言，其它系统例如3D 室内场景建模[3][7]只需在大量的RGB-D匹 

配后生成3D  图像。再者，因为在无其他的传感器设备的情况下，仅依靠深度 

摄像机，真实场景物体在不同角度的会有不同的三维表面信息，这也使得物体 

抓取的难度陡增。除非像机器人自动导航系统那样，再加入其它传感器设备来 

辅助定位，以提高精度。 

   机器人抓取物体任务的实现是机器人智能化的一种重要体现，单纯从二维 

视觉精度可能会受影响，加入多组传感器则会使价格成本增加，同时考虑更多 

复杂算法以适应实际场景则时间复杂度陡增。因此，目前一些先进的机器人自 

动定位抓取方案还是较难实现民用。 

1.2 本本本文文文工工工作作作 

   基于上述研究背景，本文针对桌面级刚体物体的抓取问题，利用RGB-D深 

度相机来进行场景物体可抓区域的定位。本文首先对视觉驱动的机械臂抓取进 

行综述，进而提出了一种基于深度相机的场景物体定位与抓取算法，实验表明 

抓取区域的定位效果能够指导机械臂进行抓取；本文还实现了一个基于深度相 

机的机械臂实物抓取系统，以演示机械臂抓取的过程。 

   本文的具体工作如下： 

  1.  结合深度数据和颜色数据的场景物体筛选：利用深度信息进行场景空间分 

                      2 

----------------------- Page 13-----------------------

                     第一章 绪论 

    割，使用颜色信息对区域分类判别； 

  2. 利用三维点云的配准进行物体的抓取区域的定位，通过局部点云与整体点 

    云配准以获取变换矩阵，来将模型预定义区域转换到实际点云空间； 

  3.  基于Intel Realsense F200深度相机和Dobot Magician机械臂，借助OpenCV与 

    PCL开源库实现机械臂实物抓取系统； 

1.3 论论论文文文结结结构构构 

   本文一共六章。第一章为绪论，介绍了机器人应用研究的相关背景，以及 

本文的主要工作。第二章主要介绍视觉与机器人的历史以及当前视觉伺服的研 

究进展和物体抓取研究综述。第三章描述了本文提出的基于点云配准的机械臂 

自动抓取物体方法中利用深度信息进行区域分割、利用颜色信息进行物体分类 

的方法。第四章为本文算法的核心内容，即结合RANSAC和ICP 的三维点云数据 

进行模型配准，以获取物体的可抓取区域的定位。第五章讨论了基于深度相机 

的实物抓取原型系统的具体实现，给出了在现实中使用本文方法驱动机械臂进 

行物体抓取的相关配置以及抓取效果。第六章总结全文，给出了本文的贡献、 

不足以及对未来工作的展望。 

                         3 

----------------------- Page 14-----------------------

                   第二章 研究进展综述 

                第第第二二二章章章 研研研究究究进进进展展展综综综述述述 

   机器人行为认知是通过视觉传感器令机器人具有人类的感知能力，对场景 

中的空间信息进行理解，同时对物体语义剖析的过程，是基于机器视觉，利用 

各类传感器感知、识别与理解并针对理解做出相应行为。通过机器视视觉的语 

义驱动是实现机器人行为认知的重要手段。 

2.1 机机机器器器视视视觉觉觉的的的发发发展展展 

   在20世纪50年代初，机器视觉的萌芽初现，起初是针对二维图像进行一些 

简单的图像处理工作；到了20世纪60年代，Roberts建立了“积木世界”的模型， 

通过该模型将环境限定，即环境中的物体由多面体组成，从而开创了三维立体 

视觉的研究；1977年，英国的David Marr教授从计算机科学的观点出发，结合 

数学、心理学以及神经学，首创视觉计算理论（后人将其命名为Marr三层视觉 

理解理论）[8] ，使得机器视觉走上了新纪元。然而天妒英才，Marr教授中道 

崩殂，不幸于1980年11月17 日在波士顿病死，年仅35岁。为纪念计算神经学的 

创始人David Marr教授，从1987年开始，国际计算机视觉大会（ICCV ）委员会 

以Marr教授名字命名的马尔奖（Marr Prize ）[9]成立，为计算机视觉领域的最 

高荣誉。虽然Marr教授故去，但20世纪80年代至今，计算机视觉领域蓬勃发展， 

新概念、新方法和新理论层出不穷，都是依托在Marr理论的框架下。Marr视觉 

理论已成为机器视觉领域的扛鼎框架，对计算机视觉走向产生了深远影响。 

   现如今机器视觉的发展已渗透于实际生活的各个方面，同时对机器人行为 

指导也起了关键作用，其中基于视觉伺服的机器人驱动框架就是依托于机器视 

觉的相关应用所展开的。 

2.2 视视视觉觉觉伺伺伺服服服控控控制制制的的的现现现状状状 

   如图2.1所示，视觉伺服的架构由四部分组成：任务分配、视觉系统、控制 

系统和机器人。任务分配是指需要机器人做的具体任务，即下达的指令；视觉 

系统对输入视觉信息作出处理；控制系统而是对视觉反馈信息进行处理以指导 

机器人进行操作；而机器人则需在执行任务之后将任务反馈回任务分配模块。 

其中，视觉系统的信息处理是机器人的双眼，能体现视觉伺服的智能化。 

   视觉伺服控制指利用视觉进行系统反馈以驱动控制，涉及计算机视觉、机 

器人技术和控制理论等多个领域。其中，Hutchinson等人分别于96[10] ，06[ 11] ， 

                         4 

----------------------- Page 15-----------------------

                   第二章 研究进展综述 

                  图2.1: 视觉伺服系统架构 

07[ 12]年发表三篇视觉伺服控制的综述性论文，对视觉伺服控制的研究起到了 

引导作用。设计视觉伺服系统的关键在于视觉系统处理和控制策略的实现。在 

视觉系统方面需考虑处理效果，动态性能和噪声处理等问题；而控制策略方面 

是针对系统中模型不确定性进行约束处理。这里我们着重介绍通过视觉信息驱 

动部分，即视觉系统的组成部分。 

   视觉系统主要由视觉信息元素获取和视觉信息处理两部分构成。其中，信 

息元素获取是在相机模型下，将三维空间映射到二维图像空间的过程（若使 

用RGB-D相机则可实现实际三维空间到相机三维空间的映射过程），而视觉处 

理则是利用视觉图像信息进行各类复杂处理后进行视觉反馈的过程。 

   2.2.1 典典典型型型的的的相相相机机机模模模型型型 

   表2.1展示了三种常见的典型相机模型，主要包括针孔模型[11][12]、球面 

模型[13][14]和统一化模型[15][16] 。统一化模型其实是对球面模型的扩展, 其原 

理是将各类相机的图像映射到归一化球面上去。这里还要提一下，常见的一 

些RGB-D深度相机，如Kinect、Realsense等，其相机模型大多采用的是针孔模 

                          5 

----------------------- Page 16-----------------------

                    第二章 研究进展综述 

                    表2.1: 典型相机模型 

    模型     应用范围          优点                缺点 

   针孔模型 透视相机 模型简单、图像畸变小 视野范围小、相机撤退 

   球面模型 全景相机 视野较广、旋转不变性 图像畸变大、模型复杂 

  统一化模型 各种相机 旋转不变、归一化设计 图像畸变大、模型复杂 

型，只是在此基础上通过双摄像头进行信息获取，一个获取色彩信息，另一个 

利用红外等手段进行像素级别的测距以获取深度信息，从而形成色彩图和深度 

图。 

   2.2.2 视视视觉觉觉信信信息息息处处处理理理 

   视觉信息处理主要有基于图像特征和多视图几何的方法对输入图像进行信 

息反馈。此外还有基于位置的方法，但该方法是将视觉系统直接隐含在了目标 

识别和定位中，虽然简化控制器的设计，不过须已知目标物体模型且对图像噪 

声和相机标定误差敏感。此处主要介绍图像特征法和多视图几何法。 

   2.2.2.1 图图图像像像特特特征征征法法法 

   如表2.2所示，为常用的基于图像特征的视觉信息处理方法，主要分为图像 

特征点法，光流场，图像矩，核采样以及互信息。图像特征法是利用利用SIFT 

[17]、HOG[ 18]或SURF[19]等特征进行图像视觉信息描述，在以往的视觉伺服中 

应用较为广泛且研究相对成熟，但是容易受到图像噪声和物体遮挡的影响[20] 。 

故此，学术界又提出了基于全局图像特征的方法，增加图像信息描述以增强视 

觉系统的鲁棒性，但缺点是模型较为复杂，一般还是利用局部线性化模型进行 

控制，但仅能保证局部的稳定性。光流场[21]是图像中由像素构成的二维瞬时 

场，即二维速度矢量，同时也能恢复物体三维结构和运动[22] 。光流主要应用 

还是在目标对象分割，跟踪任务以及状态恢复。互信息[23]是信息论中的一种 

有用信息度量，用于度量两个对象的相互性，也常用与机器人的视觉路径规划 

[24] 。 

   2.2.2.2 多多多视视视图图图几几几何何何法法法 

   多视图几何[25]是描述多幅图像之间的关系，能间接反映相机之间的几何 

关系。多视图几何法中常用的几类方法如表2.3所示，包括单应性（Homo-graphy ） 

[26]、对极几何（Epipolar Geometry ）[27] 以及三焦张量（Trifocal Tensor ）[28] 。 

                           6 

----------------------- Page 17-----------------------

                 第二章 研究进展综述 

                表2.2:  图像特征法分类 

       方法           优点                缺点 

       特征点 模型简单, 控制器设计方便         对噪声敏感, 容错性差 

       光流场 反映运动状态, 作为辅助控制信息 对噪声敏感, 计算复杂 

       互信息 对噪声不敏感, 可用于多模态图像 计算量较大 

当两个视图之间的极点与相对姿态非同构时，即便极点为0 ，仅能保证二者共 

线，并不能保证二者姿态一致。单应性矩阵描述的是共面特征点在两个视图之 

间的变换关系，可以唯一决定二者的相对姿态。对于非平面物体，就需要结合 

对极几何的方法进行处理，以构造平衡点附近与姿态同构的误差系统。而三焦 

张量在是一种更加通用的方法，不存在奇异性问题且对目标形状没有特殊要求。 

目前，结合对极几何和三焦张量的方法主要用于平面移动机器人的控制，在六 

自由度应用控制中还有待研究。 

                表2.3: 多视图几何法分类 

  方法  构造方式         优点                缺点 

 单应性 矩阵元素 无奇异性、计算简单 描述共面特征点、与目标模型有关 

 对极几何  极点  用于非平面场景          奇异性、平面下病态、与姿态非同构 

 三焦张量 张量元素 不限场景、无奇异性 计算量大、模型较为复杂 

2.3 机机机器器器人人人研研研究究究现现现状状状 

   机器视觉的发展也大大推动的机器人行为认知的研究。工业型机器人的工 

作范围相对固定，是基于一些定式下的任务，这种类型的机器人在西方以德国 

为首的国家中已相当成熟。但若要机器人具有更智能的功能，即服务型机器人， 

则需要更多的传感器来驱动，其中最重要的一环就是视觉传感驱动。以视觉传 

感器为主，再辅以多种其他传感器的手段已在当今服务型机器人研究领域流行 

开来。故通过视觉伺服架构进行机器人的行为驱动逐渐成为研究热点。现今国 

外有大量的以视觉驱动为主研究机器人的公司或科研机构，其中大多致力于服 

务型机器人的研究，并提供机器人的开发平台： 

   法国Aldebaran Robotics公司研制NAO机器人，配有视觉、红外、压力、超 

声波等多种传感器。利用眼睛部位安装有双目视觉系统的NAO机器人可通过边 

缘检测与场景分割完成对目标物体的识别，并建立目标物体的3D 模型；也可 

                       7 

----------------------- Page 18-----------------------

                 第二章 研究进展综述 

利用图像处理技术来获得目标物体和场景的三维信息，再计算相应的抓取姿态， 

最后对目标物体进行抓取。 

   美国的佐治亚理工学院研制出了一款家庭服务型机器人EL-E 。EL-E 的功能 

相对单一但目的明确，主要用途是在室内抓取目标物体，包括瓶子、遥控器、 

药片等等，以辅助老人或残疾人进行物体抓取任务。EL-E也安装了双目立体视 

觉传感器和一个全维视觉传感器。EL-E机器人可在用户的绿色激光指挥棒的指 

导之下接近目标，然后对场景进行检测，分割出场景中的目标物体，从而计算 

目标质心的三维坐标以及物体在场景平面的旋转角度以对其进行抓取。EL-E在 

执行抓取之前没有对目标物体的识别，是通过图像处理技术把目标物体从背景 

中分割出来的方式进行的。由于此执行抓取目标任务的特点，EL-E服务型机器 

人也被称为激光制导机器人。 

   德国卡尔斯鲁厄大学研制了一台仿生的双臂机器人ARMAR-III ，这款机器 

人主要为协助人们在厨房进行相关操作，如抓取碗、碟子等物品。ARMAR- 

III在眼部位置安装了两台独立的彩色摄像机，利用双目视觉以此来模拟人体视 

觉感知，这两台摄像机既可以单独转动也可以一起转动。许多研究人员也基 

于ARMAR-III 的开发平台实现了多种常用的视觉驱动算法。 

   国内服务型机器人要以中科大的可佳机器人为代表。第18届RoboCup机器 

人世界杯比赛中，中国科学技术大学自主研发的“可佳”智能服务型机器人荣 

获冠军。这也是我国服务型机器人首次在机器人世界杯上获得第一名，在我国 

服务型机器人领域取得了历史性的突破。 

2.4 物物物体体体抓抓抓取取取区区区域域域定定定位位位研研研究究究现现现状状状 

   当下基于视觉伺服框架的机器人行为驱动研究正如火如荼展开，智能机器 

人行为本身一个重要的任务就是实现自动取物任务，对桌面物体的定位与识别， 

并通过机械臂抓取等方式进行物体搬运工作。正如图2.1 的视觉伺服架构所示， 

在机器人实物抓取系统中，任务分配是物体抓取，视觉系统是根据RGB-D信息 

进行处理给出抓取区域在相机空间位置，控制系统是根据视觉系统反馈的相机 

空间位置进行转换后传导给机器人执行任务，机器人的执行任务就是移动相应 

机械臂进行物体抓取。本节重点在于视觉系统上的可抓取区域定位以及控制执 

行，机器人的规划移动轨迹、确定力度、方向等任务并不在本文探讨范围之 

内。 

   基于视觉的机器人抓取系统的实现在图像坐标系和机械臂坐标的标定的前 

提下，利用图像处理和视觉算法将目标进行三维表示后，再进行物体抓取位置 

确认，最后实施坐标转换，从而实现抓取。物体的检测识别以及物体抓取部位 

                      8 

----------------------- Page 19-----------------------

                  第二章 研究进展综述 

的确定都是由视觉系统所给出的，而对于坐标转换，机械臂移动等任务则是控 

制系统所需实现的。在抓取任务中，视觉定位算法是整个系统的核心所在，同 

时也是机器智能的体现。 

   机械臂抓取问题是时下研究的热点话题，本文回顾一下抓取问题的发展与 

现状： 

   早期研究中，以Bicchi[29] 以及Howard和Kumar[30]为代表，利用力封闭与 

形封闭来确保稳定的抓取。类似这种思路是通过假设物体上区域点联通，同时 

依靠较好的3D 物体模型，此类方法在实际中不具有扩展性，较难实施。另一 

条研究路线是以数据驱动[31]为导向，需要获取物体的整个三维模型，同时还 

需使用预先计算好的抓取数据，此类抓取数据有专门研究人员进行了归纳整理， 

具有代表性的就是Khoury[32] 的物体整体模型抓取集，Goldfeder[33] 的局部数据 

抓取位置集以及Brook[34] 的多物体抓取平面集。 

   使用图片数据进行自动抓取系统在进行抓取检测时，需要一个人工标定 

抓取数据集来学习出抓取定位模型，如Saxena[35]和Kehoe[36] 。随着深度学习 

[37]大浪潮的席卷，抓取检测也开始在深度学习基础上进行探索[38] ，但是深 

度学习的方法首先需要大量的训练数据，其次还需要性能较好的GPU硬件设备 

（训练和检测阶段都需要高性能硬件）。一般来说，基于深度学习的策略不太适 

合实时的应用。 

   还有一些根据物体原始模型来进行近似估计的方法：Miller等人[39]基于物 

体存在已知模型的前提下，在原始模型基础上进行抓取部位选择；Yamanobe和 

Nagata[40]则是人工标定大量物体的抓取部位；Huebner等人[41]则利用3D 数据 

点通过分界物体模型来确定可抓部分，但分解操作的时间消耗通常是巨大的。 

Miller等人[39]和Huebner等人[41]本质上都是依靠软件模拟抓取位置的生成。模 

拟的方法有时不一定能与真实的物理世界吻合。Rusu等人[42]在三维点云数据 

上预先标定几何学的标签，然后利用几何特征来对物体的可抓取部位面片进行 

生成。而Jain和Argall[43]不进行面片生成，直接在原始点云输入上操作，根据 

物体形态学分析从而扩展到任意物体抓取的方法。 

2.5 本本本章章章小小小结结结 

   本章对机器人行为认知进行了阐述，首先回顾了机器视觉的发展历史，探 

讨了视觉伺服的研究现状，对服务型机器人的典型案例作了回顾，最后对机器 

人执行物体定位与抓取任务做了综述，对当前自动机械抓取问题研究的进展进 

行了综述。 

                        9 

----------------------- Page 20-----------------------

             第三章 结合深度和颜色信息的场景物体检测 

       第第第三三三章章章 结结结合合合深深深度度度和和和颜颜颜色色色信信信息息息的的的场场场景景景物物物体体体检检检测测测 

   在进行抓取区域定位前，必须检测出物体所在的位置，才能在物体上继续 

寻找物体的可抓区域。在物体的候选区域筛选上，有两种方式可以进行，一种 

是直接用3D 点云模型去匹配整个场景点云，从而哪类物体的位姿以及物体类 

别，并且连并点云配准的任务一起完成，从而直接定位物体抓取区域，但是这 

种方式的耗时巨大，如有N 类物体就意味着要每次都匹配N 个3D 点云模型，而 

且使用点云模型去匹配整个场景的可靠度不高，场景点云其余的干扰甚大，直 

接扰动配准的精确度；第二种就是在2D下进行物体候选区域检测，以及物体的 

类别确定，在缩小范围的空间内再进行点云配准时可以更加精准。 

   另外，还要基于一个假设，机械臂在实施桌面级物体抓取时，往往相对而 

言距离很近，同时物体很少有被遮挡的情况，也就是说物体一般占据摄像头的 

大块前景区域，且深度信息较密集，易于分割。基于上述假设，我们提出了一 

种基于深度信息的分割算法，将二维图像空间分割成topk个独立候选区域，再 

通过分类器对每个候选区域进行判别，最终留下含有物体的候选区域，并附上 

的分类预测标签。 

   在本章末尾，深度分割颜色分类的方法根据不同物体的缩放比例进行特征 

提取，并进一步给出了分类准确率、以及在相同机器环境下的计算效率对比与 

分析。 

3.1 基基基于于于深深深度度度图图图像像像的的的区区区域域域分分分割割割 

     (a)  深度数据        (b)  区域增长        (c)  合并分割 

               图3.1: 基于深度图像的区域分割 

   图3.1(a)给出了Intel Realsense F200相机采集的深度数据示例。其中，该图片 

                         10 

----------------------- Page 21-----------------------

              第三章 结合深度和颜色信息的场景物体检测 

上的每一个像素点的值都代表了摄像机原点到该处实际位置之间的距离，单位 

通常为毫米，其图片像素点的明暗程度就代表距离摄像机的远近。在大部分情 

况下，深度图像上的值都是较为精确的，但是由于某些情况，深度摄像机无法 

给出准确判断，对于这些不确定的值，深度相机一般该处像素值置0 。对于0值 

像素，在图3.1(a) 中显示出来就是黑色的像素区域。造成不确定点值的原因有如 

下几种：1）像素点的实际位置离摄像机原点太远，每一种深度摄像机都会有相 

应的距离使用范围，超过这个距离范围就无法获取准确信息，类似图3.1(a) 中大 

片的背景区域就是因距离太远而置0 ；2 ）由于一些物体材质原因会使基于红外 

的相机产生漫反射，无法收到反馈信息，如图3.1(a) 中饮料瓶的上方一部分黑色 

包装就是由于这种情况所导致的。 

   基于深度相机的特性，我们在深度图像上进行区域分割策略，将空间分 

割成互相邻的区域。在算法1中，输入为深度数据:  Depth，预定主分割集的个 

数topk 。1∼3行是基于深度信息（距离）的粗略分割，选出topk个主分割集。但 

是如图3.1(b)所示，由于深度相机自身具有距离以及光学上的限制，再加上可能 

有局部点的扰动，会在主分割区域中产生细小的孔洞以及一些周边的杂碎区域。 

4∼10行就是处理孔洞与杂碎区域，将其合并到主分割集中。 

算法1     基于深度图像的区域分割 
输入:     深度数据: Depth 

        主区域个数: topk 

过程: 

 1: 在Depth上使用区域增长算法找出全黑分割集BS和深度分割集DS 

 2: 将DS 中的点集按点的数目降序排列 

 3: 从DS 中选出前topk个分割点集作为主分割集MS 

 4: for seg 2(DS −MS) [BS do 

 5: for p  2seg do 

 6:   CS  fms jms 2MS ^p in ConvexHull (ms)g 

 7:   SEG  arg minms (distance(ms); ms 2CS) 

 8:   SEG  SEG [fp g 

 9: end for 

 10: end for 

输出:     主分割集: MS 

                            11 

----------------------- Page 22-----------------------

            第三章 结合深度和颜色信息的场景物体检测 

   3.1.1 区区区域域域增增增长长长算算算法法法 

   算法1的第1步，与经典的区域增长算法[44]相类似，只不过是作用在Depth 

数据上而不是Color上，因而可以看做是一种基于距离的区域增长。其中增长 

的规则使用领近点值相差2，即深度图像上近邻点值相差2时就停止近邻合并 

（深度图像的值表示的是距离，在Realsense 中，深度图像值的单位是毫米，这 

也意味着临近点距离若大于2毫米时就不认为该两点在空间距离上临近）。在该 

规则作用下的区域增长，可以获取分割区域集合，这个集合又可以分成两个集 

合：若分割集中所有点的深度值都为0，归入全黑分割集BS ，否则归入深度分 

割集DS 。 

   对于全黑分割集BS ，我们不关心，主要精力还是集中在深度分割集DCR上。 

因此算法1的第2步在深度分割集DS根据集合中元素（即每一个分割点集）包含 

的点数个数由大到小进行排序，第3步就选出topk 个分割点集组成主分割集MS ， 

以待合并区域。区域增长的初步分割得到的效果如图3.1(b)所示（图中每块分割 

集的颜色仅为示意像素点归属的分割集，并无其他含义），可以看到有很多细碎 

的区域，以及瓶子上方一块因不可靠点而产生的缺损。因此需要进一步的合并 

来优化主分割集。 

   3.1.2 区区区域域域合合合并并并规规规则则则 

   在区域增长的粗略分割洗下，得到了含有topk个分割区域的主分割集MS ， 

接下来就需要对MS进行精炼。在算法1的第4步中，集合(DS −MS) [BS指的 

是所有分割区域集中去掉主分割集的其他分割区域集合。对于这个集合中的每 

一个点集，都需要对这个点集中的每个点进行检查。在进行检查规则阐述之前， 

需要做个定义，我们定义一个分割点集seg 的距离为该点集中每个点的深度值的 

平均值。 

   具体检查规则如下：首先，检查该点属于哪些主分割集（属于的含义是指 

该点在该分割集的凸包内）；之后从中挑出一个平均深度值与该点深度值接近 

的，最后将这个点合并到最接近的分割点集中去。 

   算法1的4∼10步的两重循环就是在检查上述规则。最终输出主分割集MS ， 

效果如图3.1(c)所示，得到topk个主分割区域，（这里topk等于5 ），相对于图3.1(b) 

来说，瓶子的黑色部分被填补了，同时边缘也合并了细小区域，相当于更加润 

滑了，使得更容易进行区域包围框选择等操作，更有助于指导下一阶段的物体 

分类任务。 

                        12 

----------------------- Page 23-----------------------

             第三章 结合深度和颜色信息的场景物体检测 

      (a)  色彩数据         (b)  候选区域         (c)  分类结果 

                图3.2: 基于颜色信息的物体分类 

3.2 基基基于于于颜颜颜色色色数数数据据据的的的物物物体体体分分分类类类 

   在区域空间分割之后，我们获取了主分割集MS 。接下来，需要利用主分 

割集MS ，颜色数据Color 以及预处理阶段训练处的SVM分类器Classifier进行 

物体类别判断，其中输入的颜色数据Color如图3.2(a)所示，其像素点与深度图 

像一一对应的，因此从深度信息上得到的区域分割集仍可作用在该图上。 

   算法2 的目的是对上一步分割结果进行分类筛选，过滤掉背景区域，留下包 

含物体的区域，并且预测出该区域所属物体的类别标签。具体需要经过 

  1.  分割区域抽取矩形包围框，并将包围框区域放缩统一大小 

  2.  抽取HOG特征，后使SVM对其进行分类判断，去除背景，并上附上预测 

    类别标签，从而获取一个f<区域,标签>g的集合 

   3.2.1 方方方向向向梯梯梯度度度直直直方方方图图图 

   在进行物体类别之初，我们先简要介绍一下进行物体分类的特征——方 

向梯度直方图（Histogram of Oriented Gradient ，HOG ）[18] 。方向梯度直方图 

（HOG ）最早提出时应用于行人检测任务[45] 。HOG特征是计算机视觉中用来进 

行物体检测与识别的特征描述子，统计图像局部区域的梯度方向直方图。HOG特 

征的提出是基于一个事实：在一张RGB 图像中，梯度方向的密度分布能较好地 

描述目标局部形状，即图像中的梯度变换可以表征该图的局部特征。HOG 的本 

质就是梯度的统计信息，以梯度的大致统计来压缩表征图片中的信息。 

   在物体抓取定位任务中，物体常常处于摄像机的相对前景区域，机械臂在 

距离物体较近时才会采取抓取定位，也就是摄像机往往距离物体较近。因此在 

物体的分类环节并不需要复杂的以深度学习为基础的物体识别方法（以CNN为 

                           13 

----------------------- Page 24-----------------------

                 第三章 结合深度和颜色信息的场景物体检测 

算法2       基于颜色信息的物体分类 
输入:       颜色数据: Color 

         主分割集: MS 

         物体分类器: Classifier 

过程: 

 1: Rect  fr jr = BoundBox(ms); ms 2MSg 

 2: Candidate  ∅ 

 3: for rect 2Rect do 

 4:  region  Extract-Region (Color; rect) 

 5:  feature  Calculate-HOG(region) 

 6:  label  Classify(Classifier; feature) 

 7:  if label ≠ 0 then 

 8:    Candidate  f< rect; label >g[Candidate 

 9:  end if 

 10: end for 

输出:       候选区域集: Candidate 

主物体识别方法常常是在解决复杂背景下的物体识别任务，同时需要大量GPU进 

行训练与测试，有实验表明以CNN为基准的方法在进行测试时，GPU和非GPU情 

况下时间消耗相差百倍），HOG则是一种很好GPU-Free 的方案，在物体类间分 

布区分度较大时，既能有效表征物体特点，又能快速分类识别。由于HOG是对 

图像局部区域的梯度统计，在几何学和光学上能保持较好的不变性；再者，在 

精细方向上抽样和较强的局部光学归一化等条件下，桌面级的物体可区分度是 

很高的（从后续的实验结果也可以看出，各类分类准确率基本都在90% 以上）。 

      (a)  HOG 中的分块机制                    (b)  梯度信息统计 

                        图3.3: HOG特征描述子 

                                 14 

----------------------- Page 25-----------------------

                 第三章 结合深度和颜色信息的场景物体检测 

    HOG 的特征提取过程大致分为如下步骤： 

   1.  输入一张彩色图片RGB ，并将其灰度化成图片Grey 

  2.  对灰度化后的图像Grey ，利用Gamma校正法进行颜色空间的归一化； 

  3.  计算灰度图像Grey 中每个像素的梯度； 

  4. 将图像划分成小细胞块（cell ），统计每个cell 的梯度直方图，即可形成每 

     个cell 的特征描述子； 

  5.  将每几个细胞块（cell ）组成一个分块（block ），一个block 的HOG特征描 

     述子就是其内所有cell 的特征描述子的集合； 

  6.  将灰度图像Grey 内的所有block 的特征描述子串联起来就是该图像的HOG特 

     征描述子 

    第2步中颜色空间归一化的目的是调节图像的对比度，降低图像局部的阴影 

和光照变化的影响，同时抑制部分噪音干扰。对灰度图像Grey进行Gamma压缩 

的公式为 
                         I (x; y) = I (x; y)gamma                (3.1) 

其中，I (x; y)为灰度图Grey上坐标为(x; y)的像素亮度； 

    第3步的计算图像横纵坐标方向的梯度不仅可以捕获图片上的纹理和轮廓 

信息，还能进一步削弱光照影响，图像中(x; y)处的梯度计算如下： 

                   G  (x; y) = H (x + 1; y) −H (x −1; y) 
                    x 

                   G  (x; y) = H (x; y + 1) −H (x; y −1) 
                    y 
                   G(x; y) = √G  (x; y)2  + G  (x; y)2           (3.2) 

                               x         y 

                                  G  (x; y) 
                   α(x; y) = arctan( y   ) 
                                  G  (x; y) 
                                   x 

上述公式中，G  (x; y)与G  (x; y)分别为(x; y)处像素的x方向与y方向的梯度幅值， 
             x        y 

G(x; y)为(x; y)处总体梯度幅值，而α(x; y)是梯度方向； 

    第4步的细胞块(cell)和第5步的分块（block ）是将图片本身进行区域划分的 

过程，如图3.3(a)所示，每4  ×4个像素（pixel ）组成一个细胞块（cell ），每4  × 

4个cell又组成一个分块（block ）。 

    在进行HOG特征描述子的计算时，每个cell 的描述子是进行如下计算：先 

将整个梯度方向平均分成9个方向块，也就是说360度分成9个桶统计梯度幅值的 

                                  15 

----------------------- Page 26-----------------------

            第三章 结合深度和颜色信息的场景物体检测 

加权值，如图3.3(b)所示，如果这个像素的梯度方向落在区间[60; 80)上，则该区 

间的方向统计桶中数值就加上该处梯度的幅值；同时，也如图3.3(b) 中所示,区 

间[60; 80)和[240; 260)是等价的都会落在同一个桶中。 

   另外，HOG特征描述子中的block与block之间是允许有重叠部分的，这就 

又需要引入一个步长（step ）的概念。如果block划分的步长和block 的边长一样， 

那么计算HOG时，block与block之间就无交集，但是一般来说，HOG 的计算往往 

需要考虑有交叠的情况，因为这样可以防止局部噪音的影响，通过周围的局部 

信息加权后弱化噪音的影响。在Dalal[ 18] 的经典HOG特征设置时就是将图片放 

缩到64 ×128大小后，规定每个cell包含16 ×16个像素，每个block包含2 ×2个cell ， 

每个cell含有9个特征，这样最终会得到一个3780维的HOG特征描述子，而这个 

描述子就可以用于物体的特征分类任务中。 

   如图3.4所示，第一行是采集的物体区域二维颜色图片，这是需要采集场景 

图片后，人工标框取出的物体区域，即需要人工标定groundtruth ，并从原始图 

片中取出该块区域；第二行是对每个物体区域图片抽取的HOG特征（其中，示 

例是先将该区域放缩到96 ×96后，再按块大小为16 ×16，细胞块大小为8 ×8，步 

进为8抽取的HOG特征）；第三行则是HOG 的可视化效果，也能直观地看出这五 

类物体在HOG特征上还是具有可分性的。可视化效果是通过MIT 的HOGgles方 

法[46]来显示的，在此仅助于直观感受HOG特征描述子的形态。 

        图3.4: 收集物体二维图片样本，抽取物体的HOG特征 

                        16 

----------------------- Page 27-----------------------

            第三章 结合深度和颜色信息的场景物体检测 

   3.2.2 支支支持持持向向向量量量机机机 

   支持向量机（Support Vector Machine，SVM ）最早由Cortes和Vapnik[47]于 

1995年提出，是一种经典的机器学习算法。在20世纪末至21世纪初，支持向量 

机风靡一时，大量学者因其优美数学形式，对其进行探索研究，在理论上有许 

多精巧证明。SVM属于监督学习模型，可用于分类与回归分析。SVM在解决非 

线性及高维模式识别中表现出诸多独有优势，尤其在小样本上SVM具有很好的 

泛化能力。 

   SVM的诞生是基于最大边界理论：平行超平面间的距离或差距越大，分类 

器的总误差越小，即在较大的裕度下分类器的泛化误差下，一个良好的分类 

是通过具有到任何类的最接近训练数据点的最大距离的超平面所实现的。如图 

3.5(a)所示，基于分界线离两边样本距离越大则分类总误差越小的理论假设，实 

线的分界线的泛化能力就弱于虚线的泛化能力，而SVM算法的目的就是要找出 

这条虚线。又如图3.5(b)所示，两边在类别的虚线（边界）上的样例就是支持 

向量。支持向量机就是依靠着这几个简单的支持向量与学习得到的参数便可复 

现实线（即分类平面），故而得名。（在实际训练数据过程中，也有个经验总结 

——支持向量的数量往往不多，若出现支持向量较多的情况，则很有可能出现 

过拟合情况）。 

   SVM将向量投射到一个高维的空间里，在空间中建立一个最大间隔超平 

面。在分开数据的超平面的两边建有两个互相平行的超平面，即在建立使两 

个与之平行的超平面间距离最大化的间隔超平面。SVM的一大亮点就是将对 

偶理论应用于传统的最优化问题中，主要有拉格朗日对偶和最大最小对偶。此 

外,SVM还能利用核函数技巧（kernel trick ）[48]使得能胜任非线性的任务，进行 

复杂边界的分类，具有很强的泛化能力。 

   在上一小节中，我们通过区域分割获取了主分割集MS ，其中包含了topk个 

主分割区域。对于每个主分割区域，我们都利用如图3.2(b)所示的白色矩形外围 

框将其框住。对于一个具体分割ms 的矩形包围框（BoundBox ）计算，遍历其上 

每个点，找出xmin ; ymin ; xmax ; ymax 后，(xmin ; ymin )和(xmax ; ymax )即为左上点和 

右下点，即可确定包围框的大小，算法2 的第1行就是计算主分割集MS 中每个 

分割的矩形绑定。 

   算法2 中的3∼10行是对包围框集Rect 中每个包围框，我们在颜色图Color 中 

将其矩形区域取出。对于这个颜色区域，先放缩到统一大小（与物体分类器训 

练时放缩的大小统一），再对其以同样的参数设置抽取HOG特征。对于这个特 

征，我们便可将其放入在预处理阶段训练好的物体分类器Classifier 中进行预 

测，得到的结果为0  ∼ n,其中0表示该区域预测为背景，预测标签为1 ∼ n表示 

                        17 

----------------------- Page 28-----------------------

              第三章 结合深度和颜色信息的场景物体检测 

       (a)  泛化能力 （Generalization ） (b)  支持向量(Support Vector) 

             图3.5: 支持向量机(Support Vector Machine) 

该区域包含物体，且物体的类别属于预测标签。其中，SVM进行多类分类时采 

用One-Versus-Rest 的策略[49] ，即每个类各自训练一个分类器，其中该类样例为 

正例，其余为负例，预测样本时每个分类器均能有得分，然后取其得分最高的 

作为该类的预测类别。 

   通过此步骤可以过滤掉干扰的背景区域，筛选出包含物体的候选区域集 

Candidate 。Candidate就是由矩形包围框和物体的类别标签组成的集合。一般 

抓取场景下，Candidate 中元素的个数通常小于等于1，也就是说目标物体一般 

只有1，或没有。 

   至此，基于RGB-D数据，通过区域空间的分割，以及特征判别，快速筛选 

出了目标物体的位置区域，并且预测了目标物体的类别，形成候选区域集。 

3.3 实实实验验验结结结果果果 

   我们在实验室环境下分别采集了五类桌面级物体：瓶子（bottle ）、茶杯 

（cup ）、易拉罐（can ）、茶壶（teapot ）和盒子（box ），另外再加上背景作为负 

类。其中我们在不同的放缩尺度上做了准确度与运算时间效率的统计。 

   如图3.6所示，分别放缩到64 ×64，80 ×80，96 ×96，128 ×128后，以块 

（block ）大小为16×16，细胞格（cell ）大小为8 ×8，步进（step ）为8抽取HOG特 

征，使用经典线性核的SVM作为分类器进行10-折交叉验证，并统计在各个放缩 

尺度上的准确度。由图中可以看出准确度差异在各尺度上并不是很大；同时， 

在计算时间消耗上，我们分别测试了64  ×64，80  ×80，96  ×96与128 ×128尺 

寸抽取特征的时间消耗，对单区域进行抽取依次为0.63 ，0.96 ，0.98 以及1.38毫 

                           18 

----------------------- Page 29-----------------------

             第三章 结合深度和颜色信息的场景物体检测 

             图3.6: 不同尺度提取HOG 的分类准确率 

秒；此外，SVM预测的时间基本均在10微秒的量级，可以基本忽略，故而最终 

选择计算花费最小的64 ×64的方式。（单区域的特征抽取时间消耗差距并不是太 

大，但由于区域分割阶段会产生topk个候选区，故需做topk次抽取，差距就会 

变成topk倍的单区域特征抽取时间差） 

3.4 本本本章章章小小小结结结 

   本章提出了基于二维信息的物体候选区域筛选方法：利用深度图像进行 

topk 的区域分割，再利用颜色数据提取HOG特征后进行SVM分类。在区域分 

割中又利用像素的深度值来进行相邻像素聚合，以及凸包隶属的策略来进行孔 

洞填补与杂碎合并，得到topk 个空间区域主分割；在物体分类阶段，利用预处 

理阶段训练好的SVM分类器对主分割集中topk 个区域依次进行判别筛选，最终 

留下含有物体的区域，并且附上预测标签。在实验中，分别从准确度与计算时 

间消耗的方面，来选择区域放缩的尺度，以达到简洁高效的二维近距离判别物 

体类别的目的，快速进入筛选出物体候选以供三维点云进行配准任务。 

                         19 

----------------------- Page 30-----------------------

           第四章 结合RANSAC与ICP 的场景物体点云配准 

     第第第四四四章章章 结结结合合合RANSAC与与与ICP的的的场场场景景景物物物体体体点点点云云云配配配准准准 

   前一章中，我们已经利用深度信息在二维空间上进行了区域分割，并对场 

景物体做出归类。本章则要展示对检测出的物体区域进行三维点云配准工作， 

从而实现物体抓取区域定位。此阶段任务一共分为四步： 

  1.  将二维候选区域点转换成三维点云数据 

  2.  在生成的点云数据上抽取三维空间特征 

  3.  基于抽取的三维空间特征使用点云配准方法使生成点云与预处理阶段的点 

    云模型进行贴合，并输出迁移矩阵 

  4. 根据迁移矩阵，将点云模型上的预定义的抓取点云映射到摄像机的空间坐 

   标中 

此外，在实验中，通过与不同的经典点云配准方法进行比较，以说明本文提出 

方法在局部点云与整体点云配准时的优势。 

4.1 点点点云云云数数数据据据生生生成成成 

   首先，在前章的基础上，我们通过区域分割以及以HOG为特征的分类筛选 

出了物体候选区域，但是这个区域仅仅是二维的信息。分割分类只是能够判别 

某个区域含有某个物体，但是并不能确定物体的抓取位置，因为在二维相机呈 

像时会因视角、光照等原因，往往难以给定启发式的规则来标定物体的可抓取 

部位。因此需要引入三维信息。三维信息的介入可以较好地弥补视角问题。深 

度相机虽然能获取物体的颜色信息和深度信息，但这仅仅是2.5D数据，要想转 

换成三维数据，即点云数据，就需要结合相机参数通过深度信息来生成点云。 

   在阐述生成点云数据之前，需要解释一下相机标定的原理。简单来说，摄 

像机标定(Camera Calibration)是从世界坐标系换到图像坐标系的过程，也就是 

求最终的投影矩阵P 的过程。相机标定分为单目和双目标定，这里我们只介绍 

单目标定，以下提到的标定都默认为单目标定。在标定过程中会出现三个基本 

坐标系：世界坐标系(World Coordinate System)，相机坐标系(Camera Coordinate 

System)和图像坐标系(Image Coordinate System) 。一般来说，标定的过程分为两 

个部分：1）世界坐标系转换为相机坐标系，是三维点到三维点的转换，需要R， 

                        20 

----------------------- Page 31-----------------------

             第四章 结合RANSAC与ICP 的场景物体点云配准 

        (a)  真空相机模型           (b)  主点与焦距       (c) 图片坐标系 

                      图4.1: 单目视觉标定 

⃗ 
t组成的外参矩阵（相机外参）；2 ）相机坐标系转为图像坐标系，是三维点到二 

维点的转换，需要内参矩阵（相机内参）。 

    图4.1(a)展示了针孔摄像机的呈像模型，其中C是相机的中心点（camera cen- 

ter ），Z轴是相机的主轴（principal  axis ），在图片平面（image  plane ）上的中 

心点p 是主轴与呈像平面的交点，即主点；从X 轴方向观察这个模型就是如图 

4.1(b)所示，相机中心点C 到主点p之间的距离就是f ，俗称焦距）。其中呈像平 

面上的x与y坐标轴和相机坐标系上的X 和Y 轴是相互平行的，相机坐标系以X ， 

Y ，Z三个轴组成的且原点在C 点，而呈像平面坐标系是以x ，y两个轴组成的 

且原点在主点p 处。如图4.1(c)所示，图像坐标系和呈像平面坐标系在一个平面 

上，但是图像坐标系的原点在图片的角上而不在主点处，与呈像坐标系存在一 

个(x  ; y )的偏移。 
   0  0 

   通过上述简单的相机呈像知识，我们可以开始进行相机标定的两步走。首 

先第一步是世界坐标系到相机坐标系的转换，世界坐标系其实是人为规定的， 

在点云生成的过程中，我们完全可以认为世界坐标系和相机坐标系是等价的， 

也就是说相机的点云数据就是相机自身坐标系下的三维空间。在需要与其他参 

考物做坐标系迁移时，世界坐标系和相机坐标会不同，这是就需要来确定外参 

矩阵，比如下一章的相机坐标与机械臂坐标的转换。这样第一步中的世界坐标 

转换已经是相机坐标了，接下来就是相机坐标到图像坐标的转换。这其中就要 

涉及相机的内参矩阵。 

    由图4.1(c)可知，相机坐标中一点(X; Y; Z )在呈像平面上对应一点(x; y)，可 

以很容易推出其对应关系为： 

                        f ·X       f ·Y 
                    x =      ;  y =                     (4.1) 
                         Z          Z 

又由于，相机坐标(x; y)和图像坐标(u; v)之间存在一个(x  ; y )的偏移，故相机坐 
                                          0 0 

                             21 

----------------------- Page 32-----------------------

                第四章 结合RANSAC与ICP 的场景物体点云配准 

标系中的(X; Y; Z )到图像坐标(u; v)的转换关系为： 

                        f ·X               f ·Y 
                    u =       + x0  ;  v =      + y0                (4.2) 
                          Z                 Z 

利用齐次坐标划归成矩阵变换的形式则有： 
                    2 3    2       3  2         3  2 3 
                                                   X 
                     u      f  0 x     1       0 
                                                  6  7 
                                   0 
                    6  7   6        7 6         7 6  7 
                                                   Y 
                  Z 6  7=  6        7 6         7 6  7              (4.3) 
                     v      0  f  y0     1     0 
                    4 5    4       5  4         5  6 7 
                                                   Z 
                     1      0  0  1         1  0  41 5 

由焦距f ，图像坐标偏移量(x  ; y )构成的矩阵K就是相机的内参矩阵（Intrinsic 
                           0 0 

Parameters ），即： 
                                 2        3 
                                  f  0  x0 
                                 6         7 
                            K =  6         7                        (4.4) 
                                  0  f  y0 
                                 4        5 
                                  0  0   1 

但是，还需考虑一点问题：相机坐标系和呈像坐标系的度量单位是距离单位m ， 

而图像坐标系是以像素（pixel ）的个数为单位的，这里就需要引入一个尺度因 

子的概念，即单位像素的实际尺寸。由于图像的尺寸往往不是正方形（例如， 

1024 ×768的图片横纵尺度因子是不同的），故需引入dx和dy两个尺度因子分别 

表示x轴和y轴的单位像素表征的实际尺寸。因此，内参矩阵K 需要做一下小修 

正，成如下形式： 
                              2              3 
                               f=dx    0   x0 
                              6               7 
                         K =  6  0   f=dy  y0  7                    (4.5) 
                              4  0     0    1 5 

    又因为Z与相机中心到该点的距离z 等价，故而根据相机标定的原理，有如 
                                    c 

下坐标转换公式， 
             2 3    2              3  2        3         2  3 
                                                         xw 
                                                 [    ] 
              u      f=dx   0    x    1       0 
                                                         6  7 
                                  0 
             6  7   6              7 6          7        6  7 
                                                  R   t  yw 
           z 6  7=  6 0    f=dy  y 7 6   1    0 7        6  7       (4.6) 
            c v                   0 
             4 5    4              5  4        5         6  7 
                                                  0  1    z 
                                                         4  5 
                                                          w 
              1       0     0    1          1 0 
                                                          1 

其中u, v为图像坐标系下的任意一点坐标。f 为相机的焦距，dx, dy分别为x, y轴 

方向的尺度因子，即表示单位像素的实际尺寸。x  , y  分别为图像的中心坐标。 
                                              0  0 

x , y , z 表示世界坐标系下的三维坐标点。z 表示相机坐标的Z轴值，即目标 
 w   w  w                                 c 

到相机的距离。中间的一个三阶单位矩阵外加全零平移向量组成的3 ×4 的矩阵 

                                   22 

----------------------- Page 33-----------------------

               第四章 结合RANSAC与ICP 的场景物体点云配准 

                       ⃗ 
仅为消除齐次的影响。R，t分别为外参矩阵的旋转矩阵和平移向量。在生成点 

云过程中，我们认为世界坐标系就是相机坐标系，那么R就是一个3阶的单位矩 
    ⃗ 
阵，t就是一个3 ×1的全0 向量，则转换公式4.6就可以简化成 
                     2 3   2            3  2 3 
                     u      f=dx   0   x0  xw 
                     6 7   6             7 6  7 
                  z  6 7=  6             7 6  7              (4.7) 
                   c v       0   f=dy  y0  yw 
                     4 5   4            5  4 5 
                      1      0     0   1   zw 

然后通过矩阵求逆等方式，可以推出(xw ; yw ; zw )的计算方式： 
                     8 
                     > 
                      x   =  z ·(u −x  ) ·dx=f 
                     > 
                       w     c      0 
                    <y    =  z ·(v −y  ) ·dy=f               (4.8) 

                       w      c     0 
                     > 
                     > 
                     : 
                      zw  =  zc 

    一般来说，相机的内参矩阵中的偏移参数x  ，y  为相机分辨率的一半（例 
                                       0   0 

如相机分辨率是640  ×480 ，x  和y  的值就分别为320和240 ）；焦距f 和尺度因 
                         0   0 

子dx，dy可以通过查阅相关型号相机的手册获取，或者如Intel Realsense深度相 

机可直接提供SDK接口去读取相机内参。但是如果相机型号未知，无法直接从 

手册获取参数信息时，就需要使用张正友相机标定法[50]来测量相机内参。 

    在确定相机内参之后，求取点云的公式4.8就只剩下相机Z轴的坐标值。一 

般的非深度摄像机是无法直接获取像素所离摄像头的距离zc                        （有相关研究是基 

于单张RGB 图片进行图像深度估计的方法[51][52] ，但还是有时还是会和实际距 

离有明显误差），而深度摄像头却可在此大展神威，轻松获取(u; v)像素处的深 

度值z  ，所有的像素组成的图片就构成深度图像，故而根据上述公式4.8可以根 
     c 

据上一章对深度图上的二维候选区域Candidate 中的所有像素点生成该区域的 

点云数据P ointsCloud 。 

4.2  快快快速速速点点点特特特征征征直直直方方方图图图 

    在生成点云数据P ointsCloud后，我们需要对其抽取三维空间的一种特殊 

描述子，以便后续过程进行点云配准。在此，我们选取了快速点特征直方图(Fast 

Point Feature Histograms ，FPFH)[53]作为特征匹配的描述快速点特征直方图是点 

特征直方图（Point Feature Histograms ，PFH ）[54] 的一种近似表示，但是抽取速 

度较后者有明显提高，同时丢失很少量的空间信息。 

    为了描述快速点特征直方图（FPFH ）就必须明确点特征直方图（PFH ）的 

概念。点特征直方图是基于点与其k邻域之间的关系以及其估计法线。简言之， 

PFH考虑估计法线方向之间所有的相互作用，捕获较好样本表面变化情况，以 

                                23 

----------------------- Page 34-----------------------

                 第四章 结合RANSAC与ICP 的场景物体点云配准 

描述样本的几何特征。因此，合成特征超空间取决于每个点的表面法线估计 

的质量。图4.2(a)所示是在计算PFH时，一个查询点pq                      的影响区域。pq        用红色 

标注并放在圆球的中间位置，半径为r ，pq 的所有k邻元素（即与点pq 的距离小 

于半径r 的所有点）全部互相连接在一个网络中。PFH描述子是计算邻域内所 

                                      2 
有点对之间关系来统计直方图，需要O(k  )的计算复杂性。如图4.2(c)所示，计 

            (a)  PFH的k近邻作用域               (b)  FPFH权值模式下的k近邻作用域 

PFH计算 

                               (c)  法线相对差计算 

                      图4.2:  点云数据的三维特征表示 

算p 和p  两点处法向量n 和n  的相对偏差，需要在一个统一的坐标系中进行，故 
   t   s              t   s 

而我们定义一个达布uvw坐标系，其坐标轴计算如下： 

                             u = ns 

                              v = (p −p  ) ×u                         (4.9) 
                                   t    s 

                             w = u ×v 

                                    24 

----------------------- Page 35-----------------------

                 第四章 结合RANSAC与ICP 的场景物体点云配准 

在uvw 的坐标系下，法向量n  与n  的相对差可以用下列三组角来表示： 
                          s    t 

                          α = v  nt 
                                  pt  −ps 
                          ϕ = u                                    (4.10) 
                                 ∥pt  −ps ∥ 

                          θ = arctan(w  n ; u  n ) 
                                         t    t 

其中，d  =  ∥p   −p  ∥ 表示p  与p  之间的欧式距离。计算k邻域内各个点对的四 
             t   s 2      t   s 

组值(α; ϕ; θ; d)，这样就把两点及其法向量相关的12个参变量（两点各自的(x, y, 

z)坐标值和法向量(n  , n  , n  )值）减少到4个。 
                   x  y  z 

    对于欧式距离特征d，有实验表明[54] ，利用2:5D 的数据上从不同的视角形 

成的点云计算出的欧式距离作为d来验证其作用时，发现除去这个特征后并未 

对PFH特征区分的鲁棒性产生影响。因此在FPFH 的特征计算中将这个进行了剔 

除，仅将α ，ϕ和θ作为特征，并将这个三元组称为简单点特征直方图（SPFH ）。 

    FPFH特征描述子的计算复杂度较之FPH 降低到了O(nk)  ，但仍然保留了 

PFH大部分的识别特性。下面简要介绍一下，FPFH 的特点以及计算过程。 

    对于点云上每一个查询点pq ，都能计算出与其相邻点的PFH描述子的四元 

组值(α; ϕ; θ; d)。但现在只需要用到其中的三元组(α; ϕ; θ),这个三元组称为简单 

点特征直方图（Simple Point Feature Histograms，SPFH ）。FPFH特征描述子由以 

下公式所确定： 

                                           k 
                                          ∑ 
                                        1     1 
              F P F H (p  ) = SP F H (p  ) +     SP F H (p )       (4.11) 
                       q           q                      k 
                                        k    wk 
                                          i=1 

其中，权值w  表示查询点p  和其某个近邻点p  在给定空间的距离，因而w                               的计 
            k            q                k                         k 

算可以由多种方式进行，如欧式距离，闵式距离等。从图4.2(b)可以看出FPFH计 

算的影响域范围：给定查询点pq                 （图中红色标出），均与k个近邻点直连（图 

中灰色标出），每个直连的近邻点又与其自身的近邻点连接，最终达到调整直 

方图权重来最终确定FPFH 的值。计算FPFH时，先通过p  与其计k近邻对来估 
                                                      q 

算SPFH的值，再对点集中的所有点进行SPFH的计算，这样就要使用其近邻点pk 的 

SPFH值来计算pq 的FPFH 的值。在图4.2(b) 中有些值对信息被计算了2次（图中的 

数字2标记），这可以看出FPFH和PFH 的一些区别： 

   1.  FPFH没有全部连接pq 的所有近邻点，可以对于图4.2(a)与4.2(b)看出，FPFH丢 

     弃了一些可能捕获p  形态学的值对信息； 
                       q 

   2.  PFH是在pq 的附近模拟一个较精确的平面，而FPFH包括了一些超出半径 

     为r 的球体范围的点（最远到2r 的距离）; 

                                    25 

----------------------- Page 36-----------------------

           第四章 结合RANSAC与ICP 的场景物体点云配准 

  3. 由于重计算权重的机制，FPFH融合了SPFH的值，同时还能重新获取某些 

    近邻点的值对信息; 

  4.  FPFH 的整体复杂性较之PFH大大降低且仅损失少量信息，FPFH才有可能 

    使用应用在实时或准实时的需求上； 

  5. 如图4.3所示，FPFH通过三元组分解，简单生成d分离特征直方图，对每个 

    特征维度单独绘制，并将其连接在一起 

经典的FPFH 的实现是统计11个子区间，即对于四个特征值，每个都将其参数区 

间分割为11个，特征直方图会被分别计算，最后合并成一个33维度的特征向量。 

               图4.3: FPFH 中的d维分离直方图 

4.3 结结结合合合RANSAC与与与ICP的的的点点点云云云配配配准准准 

   在分别计算生成点云P ointsCloud和预测类别对应模型Modellabel 的FPFH特 

征后，需要对两片点云进行配准。由于生成点云是局部点云（也就是实际场景 

的点云，摄像机只能捕获到物体的某一个面或几个面的信息），预处理阶段建立 

的三维模型是整体点云，因而配准工作需要进行两步走战略：如算法3所示，首 

先进行粗匹配，利用FPFH 的空间性加上随机一致性算法(RANSAC)的策略进行 

粗匹配，将生成点云和模型中的某几个面匹配上；再利用ICP 的逐步迭代特性进 

行调整，从而达到精确配准。 

   图4.4分别展示配准前，RANSAC ，ICP作用后的点云位置变化情况，其中 

白色的片面点云是从场景中生成的物体区域点云，而绿色的整体点云是预处 

理阶段物体模型的三维点云。图4.4(a)是物体模型点云与场景中物体区域点云 

加载到三维空间的状态；图4.4(b)是在RANSAC算法作用后的结果，可以看出 

                        26 

----------------------- Page 37-----------------------

                   第四章 结合RANSAC与ICP 的场景物体点云配准 

算法3         点云配准 
输入:            区域点云数据: P ointCloud 

              3D 物体点云数据: Modellabel 

              最小相似度: sim 

              最大可接受距离: mad 

              最大迭代次数: mi 

              接受比例: ar 

过程: 
                      ⃗           T 
  1: R  dig (1; 1; 1), t  (0; 0; 0) 

  2: repeat 

  3:   mrs  Random-Select(Modellabel ) 

 4:   pcrs  Random-Select(P ointCloud) 

  5:   similar   f<  p1; p2    >  jSimilarity (F P F H   ; F P F H ) >  sim ^p 1  2 
                                                       p 1       p2 

       mrs ^p2 2pcrs g 
          ⃗ 
  6:   R, t  Estimate-TransformMatrix (similar) 
                                                ⃗ 
  7:   consensus  fp jmin(distance(R ∗p + t; q); q 2P ointCloud) < mad ^p  2 

       Modellabel g 
  8: until jconsensus j > ar or Reach mi 
          jP ointCloudj 
           ′ 
                                    ⃗ 
  9: Model label = R ∗Modellabel  + t 

                                      ′ 
          ⃗ 
 10: Ricp , ticp  Classic-ICP (Model label ; P ointCloud) 

 11: R  Ricp  ∗R 
    ⃗           ⃗  ⃗ 
 12: t  Ricp  ∗t + ticp 

输出:           旋转矩阵: R 
                         ⃗ 
              平移向量: t 

RANSAC使得物体区域点云和物体模型的整体点云中的相应面进行贴合，虽然 

仍稍有偏差，但大致贴合在一起，实现粗匹配效果；图4.4(c)则是在ICP算法的 

作用下，进行逐步微调，最终达到收敛，使局部点云与整体点云完全贴合。 

     4.3.1  随随随机机机一一一致致致性性性算算算法法法 

     4.3.1.1  原原原理理理 

     随机一致性（Random Sample Consensus ，RANSAC ）是根据一组包含异常 

数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法， 

于1981年由Fischler和Bolles[55]最先提出。RANSAC算法经常应用于计算机视觉 

                                         27 

----------------------- Page 38-----------------------

            第四章 结合RANSAC与ICP 的场景物体点云配准 

      (a)  配准之前        (b)  RANSAC算法      (c)  ICP逐步微调 

                   图4.4: 配准各阶段示意图 

领域以解决特征点匹配问题，如双目视觉匹配与对齐。 

   RANSAC算法最早用于统计模型的拟合，其基本假设是样本中包含正确数 

据(inliers)，也包含异常数据(outliers)，这些异常数据可能是由于错误的测量、 

错误的假设、错误的计算等产生的。随着RANSAC 的逐步发展，现在也假设给 

定一组正确的数据，存在可以计算出符合这些数据的模型参数的方法。 

   RANSAC与其说是一种算法，不如说是一种策略或者框架，其精髓就在于 

根据不同需求定义inliers和outliers 的辩证关系，从而通过逐步迭代来进行模型匹 

配。RANSAC 的基本思想可以描述成如下步骤： 

        (a)  含噪音点的原始数据           (b)  经过RANSAC拟合的直线 

            图4.5: RANSAC算法可适应含噪音的数据集 

  1.  在数据集合P 中随机抽取n元素组成一个P 的子集S         （jS j <  jP j），利用抽 

    取的子集S初始化模型M ，其中n为初始化模型参数所需的最小样本数； 

                           28 

----------------------- Page 39-----------------------

             第四章 结合RANSAC与ICP 的场景物体点云配准 

  2.  集合SC 是集合P 中去除子集S后余下的集合，即SC           =  P  −S ，抽取的子 

                                                ∗  ∗ 
    集S 以及集合SC 与模型M 的误差小于阈值τ 的样本集构成S             ，S  就是所有 

    内点（inliers ）的集合，即S 的一致集合（Consensus Set ）； 

  3.  若jS  ∗j ≥ N ，N 是模型接受度的阈值，即如果内点集合中点数大于等 
    于N ，则认为是已获取正确的模型参数，并利用集合S∗ 中的点重新计算新 

    的模型M ∗ ；否则，重新随机抽取新的S重复以上过程。 

  4. 若迭代I 次后（I 为最大迭代次数阈值），依然无法找到一致集合S∗ ，则立 

    即停止，算法失败。 

    图4.5展示了RANSAC 的一个简单应用，4.5(a)是一些含有噪音点需要进行 

拟合的数据，图4.5(b)就是通过RANSAC不断迭代拟合出的一条符合模型的直 

线，成功避开了噪音点的影响。图4.5(b) 中蓝色点是内点（inliers ），红色点是噪 

音点（outliers ）。 

   4.3.1.2 应应应用用用 

   在局部点云与整体点云的匹配环节，我们也使用RANSAC 的策略进行匹配。 

如算法3所示，先分别从点云模型和生成的点云中选取一些点形成mrs和pcrs 点 

集；之后根据两个点集之间各自点上的FPFH特征进行相似度匹配，相似匹配 

过程中需要依靠输入阈值sim         （0  ≤ sim  ≤ 1）进行筛选，相似度大于sim的就 

加入相似点对集similar ，在这里，相似度Similarity 的计算利用的是余弦距离 

作为两个点FPFH特征的相似度；接下来，根据上步得到的相似点对集similar ， 

只要similar 中的点对数大于3，就可以进行迁移矩阵的估算（similar 的数目小 

于等于3，无法估计，只能输出单位矩阵），估计迁移矩阵的过程是先求解三 

个轴的旋转角θ  ; θ  ; θ  [56]后，根据旋转角求解旋转矩阵R，之后计算重心位移 
           x y  z 

              ⃗ 
偏差作为平移向量t ；如算法3 中第7步所示，获取估算的旋转矩阵R和平移向 
  ⃗ 
量t后，可将模型点云上的点通过迁移矩阵作用后的点在场景点云中搜索离其最 

近的点：如果此两点间的距离小于最大接受距离阈值mad ，则将该点加入一致 

性集合consensus 中。3的第8步就是检验一致性集合consensus 中点的个数与场 

景点云P ointCloud 中的点数比率是否超过可接受比率ar ，或者RANSAC算法已 

经达到最大迭代次数跳出。若达到可接受比率ar ，则跳出循环，继续进行下一 
                                                         ⃗ 
步ICP算法；若达到最大迭代次数跳出，则说明RANSAC算法失败，则R和t分 

别为3阶单位矩阵以及1  ×3的全零向量，之后也将进行ICP算法。可以看出如 

果RANSAC算法失败后，算法3将会退化成经典的ICP算法。如图4.4(b)所示，即 

为RANSAC算法粗匹配后的效果。 

                             29 

----------------------- Page 40-----------------------

                 第四章 结合RANSAC与ICP 的场景物体点云配准 

    4.3.2  迭迭迭代代代近近近邻邻邻点点点算算算法法法 

    4.3.2.1  原原原理理理 

    迭代近邻点算法（Iterative Closest Point ，ICP ）是一种基于四元数的[57] ， 

以点集对点集配准方法为基础的曲面拟合算法，最早由Besl和Mckay[58]于1992年 

提出。此后经过二十几年的发展，由后人不断完善与补充： 

    Chen和Medioni[59] 以及Bergevin等人[60]提出了Point-to-Plane搜索就近点的 

精确配准方法；Rusinkiewicz和Levoy[61]提出了Point-to-Projection搜索就近点的 

快速配准方法；Park和Subbarao[62]提出了Contractive-Projection-Point搜索就近 

点的配准方法。此外，Andrew和Sing[63]在ICP算法中考虑三维扫描点的纹理色 

彩信息提取点纹理信息的配准方法；Natasha等人[64]分析ICP算法中的点云数据 

配准中的稳定取样的问题。 

                                                             3 
    下面简要介绍一下ICP算法的基本原理：假设在三维空间R  中存在两组各 

                                                                  3 
含有n个坐标的点集PL和PR ，分别为PL                   = fpl1 ; pl2 ; ··· ; pln jplk 2 R  g，PR = 

                        3 
fpr 1 ; pr2 ; ··· ; prn jprk 2R  g。ICP 的目标也是将通过空间变换使两个点集中的点 
                                          ⃗ 
进行一一对应，输出旋转矩阵R与平移向量t ，即三维空间点集PL在经过三维空 

间的旋转平移变换后，与PR 中的点一一对应，其点变换式为 

                                             ⃗ 
                             p⃗rk = R ×p⃗lk  + t                      (4.12) 

    在ICP配准算法中，空间变换参数向量X 可表示为X  = [q  ; q  ; q  ; q  ; t  ; t  ; t ]T  。 
                                                         0  x  y  z  x y  z 

参数向量X 中四元数参数满足约束条件为：q2 + q2 + q2 + q2                    = 0 。根据迭代的初 
                                          0    x   y   z 

值X  , 由式4.12可计算出新的点集P  : 
    0                           I 

                                                ⃗ 
                       P  = P  (X  ) = R(X  )P + t(X  )               (4.13) 
                        I    0   0        0        0 

其中，P  表示进行第I 次迭代后的点集，其下标I表示迭代的轮数，参数向量X 的 
        I 
初始值X0     = [1; 0; 0; 0; 0; 0; 0]T ，P 则表示初始点集。 

    根据以上的数据处理方法，迭代近邻算法可以分为以下七个步骤： 

   1.  输入两组点集PL 与PR ，以及迭代差阈值τ 和最大迭代次数IMAX 

   2.  根据点集PL 中的点plk ，在点集PR 上搜索其相应的最近点Prk ； 

   3.  计算两个点集的重心，并进行点集中心化以生成新点集； 

   4. 由新点集可计算出正定矩阵N ，再分解出N 的最大特征值及其最大特征向 

     量； 

                                     30 

----------------------- Page 41-----------------------

              第四章 结合RANSAC与ICP 的场景物体点云配准 

  5. 由于最小残差平方和的旋转四元数与最大特征向量是等价的，故可将四元 

    数转换为旋转矩阵R ； 

                              ⃗ 
  6.  估算出旋转矩阵R后，平移向量t可通过两个点集的重心点的坐标差来确 

    定； 

  7. 根据式4.13 ，计算点集Plk 旋转平移后，得到点集P ′         。记点集Plk 与P ′  计算 
                                           lk           lk 

    累加距离差值为f        ，以两次累加距离差的绝对值∆f  = jf           −f  j作为迭 
                  k+1                            k+1   k 

    代判断数值； 

  8. 当∆f  < τ  或者迭代次数大于IMAX 时，停止迭代，输出旋转矩阵R与平移 
        ⃗ 
     向量t ；否则重复2至7步。 

    4.3.2.2 应应应用用用 

    在本文中，ICP算法主要作为粗匹配之后的逐步迭代微调的过程。在算法 

3 中的第11步应用经典的ICP算法[58]做微调，具体原理细节已在上一章阐明。 

需要注意的是，在该进行该步骤前，输入不是原始Modellabel ，而是经过粗匹配 
后的Model′label 的位置，因而需要利用粗匹配中的RANSAC得到的旋转矩阵R与 

       ⃗ 
平移向量t进行变换，该变换是针对Modellabel 中的所有点进行的，得到了一个旋 
转平移过后的点集Model′label ，该点集即为粗匹配后的位置。ICP算法就是将粗 

匹配的结果Model′label 与场景中的区域点云数据P ointCloud做微调。其中，在 

算法3的第11行中，ICP算法的最大迭代次数IMAX 在这一步中设为100，迭代差 
          −9                                   ⃗ 
阈值τ 设为10    。最终输出ICP算法的旋转矩阵Ricp 与平移向量ticp 。 

    在算法3的最末两个步骤（12-13行）则根据ICP 的结果，将旋转矩阵和平移 
                                                    ⃗ 
向量进行微调，得到最终的配准结果。通过旋转矩阵R和平移向量t可以轻松地 

将该模型Modellabel 上的预标定的抓取区域经过空间变换后，放在深度相机的点 

云空间中。如图4.4(c)所示，即为ICP算法微调后的效果，实现局部点云与整体 

点云的贴合。 

4.4  实实实验验验结结结果果果 

    为了验证配准算法的有效性，我们采集了五类常见的桌面级物体作为候选 

配准物体集，然后对其进行3D 模型的扫描以及利用上一章所训练的SVM物体 

分类模型进行物体的分类工作。 

    此部分的数据集是利用深度相机IntelRealsenseF 200进行采集彩色图和其 

相应的深度图，每个类大约都300张，背景的杂类1500张左右。其中2D 的彩色 

                              31 

----------------------- Page 42-----------------------

             第四章 结合RANSAC与ICP 的场景物体点云配准 

      (a)  地面实况          (b)  瓶子(bottle)     (c)  杯子(cup) 

      (d)  罐子(can)       (e)  茶壶(teapot)     (f)  盒子(box) 

             图4.6: 地面实况（Groundtruth ）与配准结果 

图像在前一章中人工标定过物体所在区域，由于该实验只要为了测试物体区域 

的配准情况，故直接拿来训练物体区域的分类器（训练集即为测试集），这样 

在进行配准前基本可确定物体区域是正确的。因为配准工作是基于物体分类的 

结果，一旦物体分类出错则配准肯定会出问题，所以在物体分类正确的前提下， 

我们进行三维模型配准的实验。 

   如图4.6(a)所示，在识别出瓶子的所在区域后，进行配准后，蓝色的矩形 

框为本文提出方法的预测区域的包围框，而红色的矩形框就是在进行算法之前 

根据领域知识，人工对物体标定可抓取区域作为地面实况（Groundtruth ）。图 

4.6(b)∼(f)分别是瓶子（bottle ）、杯子（cup ）、罐子（can ）、茶壶（teapot ）和盒 

子（box ）的配准结果，其中紫色点群是物体模型在2D 图像上的映射，黄色点 

群是模型上预定义的抓取区域在2D 图像上的映射，蓝色的矩形框就是能包住黄 

色点群的最小包围框（boundbox ）。蓝色矩形框就是本文算法给出的结果。 

   本实验评价标准采用Jaccard相似度来验证地面实况的红色矩形框与预测的 

蓝色矩形框的交合情况。其中，Jaccard相似度的定义如下： 
                            jP ∩ Gj 
                              ∪ 
                         J =                          (4.14) 
                            jP  Gj 
其中P 是算法预测出的矩形区域，G是地面实况标定的区域，P ∩ G表示矩形P 与G的 

相交部分的矩形，而P ∪ G表示P 与G的并合起来的大矩形，jX j则表示矩形X 的 

                            32 

----------------------- Page 43-----------------------

              第四章 结合RANSAC与ICP 的场景物体点云配准 

                   图4.7: 评价标准-Jaccard相似度 

面积。如图4.7所示，中间蓝色的矩形区域就是P ∩ G，P ∪ G则是图中的最大的 

黑框矩形。Jaccard相似度就是蓝色矩形与黑框矩形的面积之比。 

              表4.1: 提出方法与其他基于ICP方法的比较 

          类别      提出的方法 经典ICP        带法线ICP    非线性ICP 

       瓶子(bottle)  0.657     0.309    0.307     0.316 

       盒子(box)     0.619     0.225    0.241     0.289 

       罐子(can)     0.597     0.445    0.372     0.329 

       茶杯(cup)     0.695     0.042    0.132     0.177 

       茶壶(teapot)  0.658     0.165    0.237     0.215 

    实验是在五个类上测试，表4.1 中的数值是该类的平均Jaccard相似度，这 

里的平均就是直接取平均数，最终得出结果。从表4.1可以看出，本文提出的 

RANSAC+ICP 的点云配准方法在于其他纯基于迭代近邻点的方法有明显的提 

高，说明能一定程度上解决局部点云与整体点云的配准问题，从而规避了ICP- 

based等方法的弱势，在准确度上可以属于供参考候选的方法；同时该方法消耗 

的时间很少，且无须借助GPU等硬件，仅在Intel(R) Core(M) i3-2130 CPU@3.40 

GHz处理器下在2s 内能预测出物体的抓取位置。可以胜任准实时的任务。本文 

                              33 

----------------------- Page 44-----------------------

            第四章 结合RANSAC与ICP 的场景物体点云配准 

提出的点云配准的定位抓取法虽然在预处理部分略显繁琐，但在实际应用中是 

一种具有较高精确度且价格低廉（仅需深度摄像机和普通的PC机即可）的解决 

方案。 

4.5 本本本章章章小小小结结结 

   本章提出了一种基于刚体的局部点云与整体点云的配准方法，先利用RAN- 

SAC  算法进行粗匹配后，将局部点云与整体点云相应的面粗略匹配上，再利 

用ICP 的特性可以进行逐步微调，从而达到精确配准。之后简单通过相机的坐 

标转换将点云坐标转换为二维坐标，从而实现物体抓取区域定位任务。实验中， 

我们使用Jaccard相似度衡量匹配的准确度，实验结果也表明本文提出的方法具 

有较好的抓取定位效果。 

                          34 

----------------------- Page 45-----------------------

           第五章 基于深度相机的机械臂实物抓取系统实现 

     第第第五五五章章章 基基基于于于深深深度度度相相相机机机的的的机机机械械械臂臂臂实实实物物物抓抓抓取取取系系系统统统实实实现现现 

5.1 算算算法法法框框框架架架 

   虽然机器人领域如火如荼地发展，目前机器人研究已经取得相当优异的成 

果，机器人自动抓取方法也层出不穷，但是离真正的智能机器人普通家庭民用 

化还是有一定距离。在自动抓取任务中，物体抓取的准确度、时间效率和价格 

成本天生就是一组矛盾体。 

   基于物体抓取区域定位的研究现状，我们致力于开发一种廉价高效且准确 

度可接受的抓取区域定位抓取系统。本文提出一种基于深度相机，以区域分割、 

物体分类和点云配准为基础的物体抓取区域定位算法。其主要思想是，基于深 

度摄像机采集RGB-D数据，通过纯RGB数据（即二维数据）来检测出物体的所 

在区域以及物体所属的类别，之后再通过生成点云与点云模型进行匹配后确定 

物体的可抓取区域。因此，我们需要先对预抓物体进行3D 点云模型建立（可 

以通过扫描或软件模拟的方式），之后根据我们对物体可抓区域的认知，在物体 

的3D 点云模型上标记处物体的可抓取区域，即在模型上标记可抓取区域的点 

云数据；同时还需要一个能在二维上判别物体类别的分类器。进行这两步预处 

理后，就可以利用深度摄像机采集场景的RGB-D数据进行物体抓取区域定位算 

法，依次经过区域分割，物体分类，点云配准以及空间坐标映射等步骤确定可 

抓取区域。物体抓取区域定位算法的整体框架如图5.1所示。 

   5.1.1 预预预处处处理理理阶阶阶段段段 

   在进行物体抓取区域定位算法前，需要进行两个预处理步骤：训练物体分 

类器和物体3D 建模。其中，物体分类器是利通过抽取二维特征以及线性分类 

器进行分类，而物体 3D 建模则需要通过3D 扫描仪或一些软件创建模型，如 

3DMAX，本文中采用3D扫描仪的方式进行点云模型建立。 

   5.1.1.1 物物物体体体分分分类类类器器器训训训练练练 

   物体分类器的训练是采集场景图片中的预定义物体，抽取HOG特征后训练 

一个SVM判别器作为n类物体的分类器。其中，第1 ∼n类物体表示物体相对应 

的标签，第0类物体则表示背景类，即不属于任何一个物体类别。每个物体的训 

练样本需要人工从不同角度采集场景图片后，标出所处位置框并对其打上相应 

                         35 

----------------------- Page 46-----------------------

第五章 基于深度相机的机械臂实物抓取系统实现 

  图5.1: 物体抓取区域定位算法整体框架 

                 36 

----------------------- Page 47-----------------------

            第五章 基于深度相机的机械臂实物抓取系统实现 

的label （属于哪类物体）；同时背景类需要随机地在非标定框区域随机采集不同 

大小的样例。最终形成训练集后，再放入线性核的SVM分类器中进行训练。 

   如图5.2所示，每一行均为从不同角度采集的某一类物体，然后从这些图中 

抠出物体区域作为训练集进行训练。 

                   图5.2: 采集到的数据集 

   5.1.1.2 物物物体体体3D 建建建模模模 

   事实上，物体3D 模型建立可以有多种方式，但本文中选择使用直接使用 

3D 扫描仪建立物体的点云模型。此步骤具体细节与本文无太大关系，因而略 

去。在获取物体的点云模型之后，我们需要对物体本身的可抓取区域认知，人 

为地标记物体模型上可抓取的区域。如图5.3所示，第一行即为五类物体通过扫 

描仪获得的点云数据模型，第二行中物体上的红色区域则是在物体上人为标定 

的可抓区域。 

   5.1.2 抓抓抓取取取定定定位位位阶阶阶段段段 

   在完成了上述预处理阶段之后，我们就可以通过深度相机实时获取RGB- 

D数据后进行物体的抓取区域定位。抓取定位阶段主要分为四个阶段：区域分 

割，物体分类，点云配准与坐标映射。其中，输入数据是RGB-D数据，最终输 

出图片中物体的可抓取位置。 

                          37 

----------------------- Page 48-----------------------

            第五章 基于深度相机的机械臂实物抓取系统实现 

  1: 茶杯(cup) 2: 罐子(can) 3: 茶壶(teapot) 4: 盒子(box) 5: 瓶子(bottle) 

            图5.3: 在3D 点云模型上标记抓取区域位置 

   5.1.2.1 区区区域域域分分分割割割 

   当输入RGB-D数据，我们首先需要去二维图片的空间进行分割，我们仅在 

深度信息上基于区域增长算法，可以获取两个集合，一个是不同深度区域的集 

合，另一个是全黑区域的集合。不同深度的区域是指在深度图像上利用区域增 

长算法会将图片按照不同深度分割成不同区域，每个区域中距离都是相近，也 

就是说每次区域增长的扩展规则是按邻近点距离在某阈值范围内就扩展的规则； 

全黑区域即不确定区域，由于深度相机本身的限定，过远或过近的距离，深度 

信息都无法获取；同时由于一些物体表面材质的问题会产生强烈的漫反射，这 

种情况下同样无法获取深度信息。对于不可获取点的区域在图像上的显示就是 

全黑部分。 

   之后，通过挑选有代表性的一些深度区域，称之为主区域集合，将其他区 

域中被主区域凸包所包含的点进行合并，最终输出一个主区域集合。 

   5.1.2.2 物物物体体体分分分类类类 

   区域分割后获取了主区域集合，对于这个主区域集合中的每个区域都需要 

进行物体的类别判断。判断的依据就是依靠预处理阶段训练生成的物体分类器： 

若该区域中有物体则分类器会相应地输出预测的物体标签；若区域不包含物体， 

则分类器输出0，表示该区域属于背景类。因此，主区域集合通过分类器的筛选 

后，剩下的就是含有物体的区域集合，这个集合内每个区域都有相应的物体类 

别标签，同时我们也将这个区域集合记作，候选区域集（一般而言，候选区域 

集里只有一个区域，也就是说一个场景图片里只含有一个物体） 

                          38 

----------------------- Page 49-----------------------

             第五章 基于深度相机的机械臂实物抓取系统实现 

   5.1.2.3 点点点云云云配配配准准准 

   对于候选区域集，对每个候选区域生成点云模型，然后根据候选区域预测 

的类别标签加载对应的预扫描三维点云模型。在生成点云与点云模型之间利用 

点云配准的方法进行配准。由于该配准任务属于局部点云与整体点云配准问题， 

基于ICP等经典算法效果不是很好，因此我们采用了RANSAC与ICP结合的方式 

进行配准：先依靠RANSAC进行粗匹配，将局部点云与点云模型相应面配对上， 

之后再依靠ICP进行局部微调，从而得出点云配准的变换矩阵。 

   5.1.2.4 坐坐坐标标标映映映射射射 

   得到变换矩阵后，我们将预定义的抓取位置通过变换矩阵转换可以得到抓 

取位置在实际点云空间的位置信息（即以摄像机焦点为原点的坐标系的位置）。 

在实际抓取应用中，需要将此坐标与机械臂的坐标进行转换，转换的过程需要 

利用棋盘标定将相机坐标与机械臂坐标做迁移矩阵估计。 

5.2 软软软硬硬硬件件件实实实施施施 

   实物抓取系统硬件配置由一台桌面级机械臂，一个深度摄像头和一台普 

通PC机构成，其中本实验采用的设备清单如下: 

                  表5.1: 抓取系统所使用的器材 

          设备名                产品型号 

         处理器  Intel(R) Core(M) i3-2130 CPU@3.40GHz[65] 

         深度相机 Intel Realsense F200[66] 

         机械臂  Dobot Magician [67] 

   软件实现部分还需要借助一些额外的开源库或相关设备公司开发的SDK， 

具体清单如下： 

   其中，Intel Realsense F200与Dobot Magician分别是Intel公司和越疆科技制 

造的智能硬件。Intel Realsense F200如图5.4(a)所示，是一款以毫米为深度单位， 

捕获近距离深度的摄像头（可测深度范围为20mm  ∼ 1200mm ），且体型精巧， 

功耗低[2] ，无须额外电源，PC端的USB3.0接口直接可以供电驱使其工作；而 

图5.4(b)是Dobot Magician 的产品图，Dobot Magician则是一款多功能桌面级机械 

臂，支持物体抓取，模拟写字以及3D 打印[68]等诸多功能。 

                            39 

----------------------- Page 50-----------------------

            第五章 基于深度相机的机械臂实物抓取系统实现 

                  表5.2: 抓取系统所需依赖库 

            软件库                   功能 

       DobotDll        上位机驱动机器臂运行的接口 

       Intel Realsense SDK 提供读取深度图片、点云数据等函数 

       OpenCV          提供物体分割与识别所需函数 

       Point Cloud Library 提供基础点云读写与配准函数 

         (a)  Intel Realsense F200  (b)  Dobot Magician 

                   图5.4: 深度相机与机械臂 

   Intel Realsense SDK和DobotDll分别是相应公司开发的驱动硬件的工作的软 

件开发接口。OpenCV[69]是由Intel公司主导的计算机视觉领域的知名开源库， 

已在广泛硬件上有相应的版本。OpenCV 中含有大量的图像处理，视觉算法以及 

机器学习的库函数；本系统中二维图像上的分割与分类任务就是在OpenCV库 

函数的基础上实现。Point Cloud Library(PCL)[70]也同样是在点云数据领域具有 

扛鼎地位的开源软件库，本身集成了大量的点云相关的特征描述子和基础算法； 

本系统的点云配准算法就是依赖PCL 的基础库所实现的。 

   部署好系统所需的软硬件环境后，需要确定机械臂的坐标以及标定摄像机 

坐标与机械臂坐标进行转换，之后设定好抓取参数，就可以实施抓取实践。本 

章接下来篇幅详细介绍实际抓取的相关设定以及演示抓取测试效果。 

5.3 坐坐坐标标标变变变换换换 

   在进行抓取测试之前，先要确定摄像机坐标系和机械臂坐标的相关关系。 

其实利用的原理就是空间同名点对应法，空间同名点是指两个不同坐标系下的 

                           40 

----------------------- Page 51-----------------------

            第五章 基于深度相机的机械臂实物抓取系统实现 

点对应的是空间中相同的一点。但是机械臂和相机坐标系的同名点本身上不易 

寻找的，故需借助棋盘作为中介来确定空间同名点。首先，机械臂通过棋盘的 

坐标点来确定自身位置的平移关系，然后借助棋盘来确定机械臂和摄像机两者 

间的关系，通过找出棋盘上黑白相间的角点作为同名点来估算机械臂与摄像机 

坐标系的旋转平移关系，即相机外参矩阵的标定。 

   5.3.1 棋棋棋盘盘盘坐坐坐标标标 

   如图5.5所示，我们制作了一张实际边长为71mm 的棋盘，其中我们以毫米 

为单位，因为深度相机也是以毫米为单位。定义内圈的4  ×4 的定点为(0; 0)， 

16个点的坐标分别依次如图5.5排列。稍后会解释为何坐标原点要从内圈的4  × 

4点中开始。至此就完成了棋盘坐标的确定，棋盘上的内圈角点就是空间的同名 

点，机械臂和相机坐标之后都需要以其为准。 

               图5.5: 实际边长为71mm 的5 ×5棋盘 

   5.3.2 机机机械械械臂臂臂坐坐坐标标标 

   机械臂Dobot Magician是一个具有四个关节自由度的机械臂，自身以底座原 

点为坐标原点的坐标系，同时有自带的Dobot Studio控制软件，有坐标和关节两 

种模式进行驱动。如图5.6(a)所示，关节1∼3分别能驱动机械臂在三维空间上的 

                           41 

----------------------- Page 52-----------------------

             第五章 基于深度相机的机械臂实物抓取系统实现 

运动，关节4是机械臂的转头旋转控制l ；同时也有坐标模式可以轻松地根据输 

入坐标移动。我们按照5.6(b)所示将机械臂头移动到坐标原点后，记录下机械臂 

本身的坐标X  ; Y  ; Z  ，则这个坐标就是机械臂与棋盘坐标的原点偏移值。由于 
          0  0 0 

机械臂与棋盘之间不存在旋转关系，故机械臂与棋盘间只需简单的平移，即有 

如下关系： 

              (X  ; Y ; Z ) = (X ; Y ; Z ) + (X  ; Y  ; Z ) (5.1) 
                A  A  A     C  C  C     0  0 0 

其中(X   ; Y ; Z )表示机械臂的坐标，(X      ; Y ; Z )是棋盘坐标，(X  ; Y  ; Z )则 
      A  A A                    C  C  C              0 0  0 

是各自轴方向的偏移值。 

       (a)  Dobot Studio 控制机械臂坐标       (b)  机械臂移动至原点 

                    图5.6: 确定机械臂原点坐标 

    5.3.3 相相相机机机坐坐坐标标标 

    通过OpenCV 的ﬁndchessboard函数可以找出黑白棋盘中的交界点，其原理就 

是通过角点检测[71]之后过滤不相干点最后留下具有空间逻辑关系的一组点， 

这组点就是我们需要的棋盘点。也正因如此，棋盘往往使用内圈点，因为外围 

点较难定位（有诸多干扰）。如图5.7所示，每个带圆圈的点就是检测出的5 ×5棋 

盘角上的16个交界点，其中红色的一排所在处就是第一行点的坐标，之间的连 

线关系就是二维空间点存储的对应关系，只需按位复原与棋盘的坐标一一对应 

即可。 

    由上一章公式4.6 以及4.7可知，我们在计算相机的点云空间坐标时其实是忽 
            ⃗ 
略相机外参R和t 的，认为世界坐标系和相机坐标系是统一的。但在这里要与机 

械臂做坐标转换时，就需要将棋盘上的点作为世界坐标系。于是我们得到如下 

                              42 

----------------------- Page 53-----------------------

              第五章 基于深度相机的机械臂实物抓取系统实现 

                图5.7: 通过OpenCV找到棋盘上的交接点 

关系： 
                       2  3            2  3 
                       XP              XC 
                            [    ] 
                                  −1 
                       6  7            6  7 
                       6  7            6  7 
                        YP   R  t      YC 
                       6  7         =  6  7                  (5.2) 
                       6  7            6  7 
                        Z    0  1      Z 
                       4  5            4  5 
                         P               C 

                        1               1 

根据OpenCV 的solvePnP函数可以求解空间同名点对应关系，其原理是当同名点 

对数大于3时，解一个12未知量的方程组的近似解。一般而言，同名点对数越 

多，求解的精度越高。 

                            ⃗ 
    估算出旋转矩阵R和平移向量t后，由公式5.1可知，摄像机坐标与棋盘坐标 

有偏移量关系： 
                   2  3            2  3   2  3 
                    XP             X0      XA 
                         [   ] 
                              −1 
                   6   7           6  7   6   7 
                   6   7           6  7   6   7 
                    YP   R  t       Y0     YA 
                   6   7        +  6  7=  6   7              (5.3) 
                   6   7           6  7   6   7 
                    Z    0  1       Z      Z 
                   4  5            4  5   4  5 
                     P               0      A 

                     1              1       1 

(XP ; YP ; ZP )的计算可由公式4.8计算得出，最终完成摄像机上任意一点有深度 

信息的像素坐标到机械臂坐标系的转换。 

    经过我们的误差检验，此标定方法的误差基本在8mm  ×8mm  ×8mm范围 

内，而Dobot Magician本身的抓取口径约3cm，是在误差可接受范围内。可以进 

行桌面级物体的抓取。 

5.4  抓抓抓取取取实实实践践践测测测试试试 

    由于Dobot Magician 的抓取头部无法进行360度的旋转，只能垂直方向抓取， 

                                43 

----------------------- Page 54-----------------------

           第五章 基于深度相机的机械臂实物抓取系统实现 

故我们采用常见的纸杯作为桌面抓取的效果演示，所用纸杯如图5.8(a)所示。如 

图5.6(b) ，根据前一节所使用的方法定位机械臂的原点位置为(147:20; 6:79; −28:03) 。 

同样在进行抓取之前，我们对纸杯进行预处理操作：采集了二维从颜色数据 

集训练分类器Classifier ，使用3D  扫描仪，对其进行建模，生成 3D  点云模 

型Model ，同时标注水杯的杯口区域的一圈点云作为模型的抓取区域Grasp 。标 

定摄像机和机械臂坐标后，对于分割和配准算法设定参数：分割算法中的topk 

设定为6 ；配准算法中，设定最小相似度sim为0:88，最大可接受距离mad为2:5， 

最大迭代次数mi为10000，接受比例ar为20% 。 

   (a)  演示物体——纸杯   (b)  抓取之前状态      (c)  抓取之后状态 

                  图5.8: 实体抓取演示效果 

   在抓取实践中，我们选取预测区域中附着在模型上的一片点云中的任意点 

进行抓取。因为模型贴合后，杯口的任意一点都可作为抓取的候选点，然后通 

过三维坐标转换驱动机械臂移动到相应位置。进行抓取测试时，每次检测到水 

杯位置后大概2秒，系统就会做出反应驱动机械臂移动至相应位置对水杯进行 

抓取，抓到水杯之后机械臂会向上提高一段距离以示抓取成功。如图5.8所示， 

5.8(b)是实施抓取之前的状态，5.8(c)则是机械臂抓取成功的状态。 

5.5 本本本章章章小小小结结结 

   本章主要介绍了实物抓取系统的相关软硬件环境配置，标定相机和机械臂 

坐标的方法以及具体定位算法的参数设置。最后，以常见的纸杯作为演示物体， 

以抓水杯的杯沿部分给出抓取的演示效果。 

                         44 

----------------------- Page 55-----------------------

                    第六章 总结与展望 

                 第第第六六六章章章 总总总结结结与与与展展展望望望 

6.1 本本本文文文成成成果果果 

   本文提出了一种全新的机械臂抓取算法模型，利用预先扫描候选物体并 

标注抓取部位，同时基于深度摄像机通过深度信息与颜色数据检测分类，结 

合RANSAC与ICP 的三维点云配准方法进行实时定位物体的可抓取位置。本文 

所提方法可概括为以下三个主要步骤： 

  1.  通过深度相机采集的深度信息进行二维图像的区域分割获取候选区域； 

  2.  针对对每个候选区域的颜色数据依次提取HOG特征，并且利用预先训练 

    的SVM分类器将物体进行快速分类； 

  3.  通过深度信息生成候选区域的点云数据，并结合RANSAC与ICP 的点云配 

    准与预先扫描的三维模型进行匹配，有效解决局部点云与整体点云的配准 

    问题 

   实验中，我们利用10-折交叉验证在五类不同的常见桌面级物体的数据集 

上，对HOG+SVM进行物体区域分类方法的可靠性进行了验证，准确度基本都 

在90% 以上；之后对这五种常见桌面级物体进行抓取区域定位测试，利用Jaccard相 

似度作为评价标准，并与一些基于ICP 的点云配准方法进行比较，结果显示我 

们的方法具有在局部与整体点云的配准上有优势，并且Jaccard相似度基本都大 

于0.6 。 

   同时本文实现了一套基于深度相机的场景物体定位与抓取系统，该系统计 

算资源消耗不大（无需任何GPU ）且效率较高，利用纸杯作为模型演示，仅 

在Intel(R) Core(M) i3-2130 CPU@3.40GHz处理器下可以在2s 内预测出抓取位置。 

综上所述，通过二维分类，三维配准的方式是一种行之有效的物体抓取部位定 

位算法。 

6.2 不不不足足足与与与展展展望望望 

   本文针对物体抓取区域定位做了一定的研究，并提出了相关算法，但提出 

的方法仍然有以下三方面的不足： 

                         45 

----------------------- Page 56-----------------------

                 第六章 总结与展望 

  1.  预处理过程的两个部分比较繁杂，需要先采集二维图片的样例，以及对物 

   体本身建模，从这点来说是需要消耗较多人力资源 

  2. 可扩展性稍弱，由于第一点中提到的预处理繁琐，因此每增加一个物体都 

   需要再进行如此操作 

  3.  抓取本身的精度有待加强，对于较为细小的物体，如粉笔，基于点云配准 

   的方法就有点力不从心 

   未来我们将继续研究新的有效的算法来实现物体抓取区域定位，提高定位 

精度，同时注重算法的效率。在二维检测的基础上，抽取出三维信息后如何利 

用原有的模型进行信息推断将会是本文进一步研究的目标。 

   进一步的研究可以从两方面考虑： 

  1.  三维方向，相似三维模型的抓取区域自推断，就是说扫描一种物体后，和 

   其类似的物体能够从原始的三维信息推断上进行三维模型上的抓取区域推 

   断； 

  2. 纯二维方向，二维图片的大类归类，利用深度学习方法，对某一类大类， 

   比如书本，能够进行识别，在大类分类之后，能否再使用类似细粒度分析 

   的手段来对该物体进行信息推断，推断的依据就是之前给神经网络训练大 

   量数据，然后对从未见过的物体也能给出可抓区域的推测解。（本文方法 

   的一大软肋就是未知物体不可抓） 

                      46 
